{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_result</th>\n",
       "      <th>score</th>\n",
       "      <th>id_student</th>\n",
       "      <th>code_module</th>\n",
       "      <th>code_presentation</th>\n",
       "      <th>mean_sum_click</th>\n",
       "      <th>total_sum_click</th>\n",
       "      <th>days_logged</th>\n",
       "      <th>material_interactions</th>\n",
       "      <th>module_length</th>\n",
       "      <th>...</th>\n",
       "      <th>region</th>\n",
       "      <th>highest_education</th>\n",
       "      <th>imd_band</th>\n",
       "      <th>age_band</th>\n",
       "      <th>num_of_prev_attempts</th>\n",
       "      <th>studied_credits</th>\n",
       "      <th>disability</th>\n",
       "      <th>grade</th>\n",
       "      <th>studied_credits_binned</th>\n",
       "      <th>date_registration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pass</td>\n",
       "      <td>82.4</td>\n",
       "      <td>11391</td>\n",
       "      <td>AAA</td>\n",
       "      <td>2013J</td>\n",
       "      <td>4.765306</td>\n",
       "      <td>934.0</td>\n",
       "      <td>40</td>\n",
       "      <td>55</td>\n",
       "      <td>268</td>\n",
       "      <td>...</td>\n",
       "      <td>East Anglian Region</td>\n",
       "      <td>HE Qualification</td>\n",
       "      <td>90-100%</td>\n",
       "      <td>55&lt;=</td>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>N</td>\n",
       "      <td>A-</td>\n",
       "      <td>201+</td>\n",
       "      <td>-159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pass</td>\n",
       "      <td>65.4</td>\n",
       "      <td>28400</td>\n",
       "      <td>AAA</td>\n",
       "      <td>2013J</td>\n",
       "      <td>3.337209</td>\n",
       "      <td>1435.0</td>\n",
       "      <td>80</td>\n",
       "      <td>84</td>\n",
       "      <td>268</td>\n",
       "      <td>...</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>HE Qualification</td>\n",
       "      <td>20-30%</td>\n",
       "      <td>35-55</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>N</td>\n",
       "      <td>C</td>\n",
       "      <td>30-60</td>\n",
       "      <td>-53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pass</td>\n",
       "      <td>76.3</td>\n",
       "      <td>31604</td>\n",
       "      <td>AAA</td>\n",
       "      <td>2013J</td>\n",
       "      <td>3.254902</td>\n",
       "      <td>2158.0</td>\n",
       "      <td>123</td>\n",
       "      <td>82</td>\n",
       "      <td>268</td>\n",
       "      <td>...</td>\n",
       "      <td>South East Region</td>\n",
       "      <td>A Level or Equivalent</td>\n",
       "      <td>50-60%</td>\n",
       "      <td>35-55</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>N</td>\n",
       "      <td>B</td>\n",
       "      <td>30-60</td>\n",
       "      <td>-52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pass</td>\n",
       "      <td>55.0</td>\n",
       "      <td>32885</td>\n",
       "      <td>AAA</td>\n",
       "      <td>2013J</td>\n",
       "      <td>2.937500</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>70</td>\n",
       "      <td>66</td>\n",
       "      <td>268</td>\n",
       "      <td>...</td>\n",
       "      <td>West Midlands Region</td>\n",
       "      <td>Lower Than A Level</td>\n",
       "      <td>50-60%</td>\n",
       "      <td>0-35</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>N</td>\n",
       "      <td>D</td>\n",
       "      <td>30-60</td>\n",
       "      <td>-176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pass</td>\n",
       "      <td>66.9</td>\n",
       "      <td>38053</td>\n",
       "      <td>AAA</td>\n",
       "      <td>2013J</td>\n",
       "      <td>3.381743</td>\n",
       "      <td>2445.0</td>\n",
       "      <td>143</td>\n",
       "      <td>88</td>\n",
       "      <td>268</td>\n",
       "      <td>...</td>\n",
       "      <td>Wales</td>\n",
       "      <td>A Level or Equivalent</td>\n",
       "      <td>80-90%</td>\n",
       "      <td>35-55</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>N</td>\n",
       "      <td>C</td>\n",
       "      <td>30-60</td>\n",
       "      <td>-110.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  final_result  score  id_student code_module code_presentation  \\\n",
       "0         Pass   82.4       11391         AAA             2013J   \n",
       "1         Pass   65.4       28400         AAA             2013J   \n",
       "2         Pass   76.3       31604         AAA             2013J   \n",
       "3         Pass   55.0       32885         AAA             2013J   \n",
       "4         Pass   66.9       38053         AAA             2013J   \n",
       "\n",
       "   mean_sum_click  total_sum_click  days_logged  material_interactions  \\\n",
       "0        4.765306            934.0           40                     55   \n",
       "1        3.337209           1435.0           80                     84   \n",
       "2        3.254902           2158.0          123                     82   \n",
       "3        2.937500           1034.0           70                     66   \n",
       "4        3.381743           2445.0          143                     88   \n",
       "\n",
       "   module_length  ...                region      highest_education  imd_band  \\\n",
       "0            268  ...   East Anglian Region       HE Qualification   90-100%   \n",
       "1            268  ...              Scotland       HE Qualification    20-30%   \n",
       "2            268  ...     South East Region  A Level or Equivalent    50-60%   \n",
       "3            268  ...  West Midlands Region     Lower Than A Level    50-60%   \n",
       "4            268  ...                 Wales  A Level or Equivalent    80-90%   \n",
       "\n",
       "   age_band  num_of_prev_attempts  studied_credits disability grade  \\\n",
       "0      55<=                     0              240          N    A-   \n",
       "1     35-55                     0               60          N     C   \n",
       "2     35-55                     0               60          N     B   \n",
       "3      0-35                     0               60          N     D   \n",
       "4     35-55                     0               60          N     C   \n",
       "\n",
       "  studied_credits_binned date_registration  \n",
       "0                   201+            -159.0  \n",
       "1                  30-60             -53.0  \n",
       "2                  30-60             -52.0  \n",
       "3                  30-60            -176.0  \n",
       "4                  30-60            -110.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "student_info = pd.read_csv('data/student_info_cleaned.csv')\n",
    "\n",
    "assessments = pd.read_csv('data/assessments.csv')\n",
    "courses = pd.read_csv('data/courses.csv') # DONE\n",
    "student_assessments = pd.read_csv('data/studentAssessment.csv')\n",
    " # IDK how to use???\n",
    "# student_info = pd.read_csv('data/studentInfo.csv')\n",
    "registration = pd.read_csv('data/studentRegistration.csv')\n",
    "student_vle= pd.read_csv('data/studentVle.csv')\n",
    "vle = pd.read_csv('data/vle.csv')\n",
    "\n",
    "datasets = {\n",
    "    'assessments':assessments,\n",
    "    'courses':courses,\n",
    "    'student_assessments':student_assessments,\n",
    "    'student_info':student_info,\n",
    "    'registration':registration,\n",
    "    'student_vle':student_vle,\n",
    "    'vle':vle}\n",
    "\n",
    "student_df = student_info.merge(registration, how='inner', on=[\"code_module\", \"code_presentation\", \"id_student\"])\n",
    "student_df = student_df.drop(columns='date_unregistration')\n",
    "\n",
    "svle = student_vle.groupby(['code_module', 'code_presentation', 'id_student']).agg({'sum_click': ['mean', 'sum'], 'date': 'nunique', 'id_site': 'nunique'}).reset_index()\n",
    "svle.columns = ['code_module', 'code_presentation', 'id_student', 'mean_sum_click', 'total_sum_click', 'unique_date_count', 'unique_id_site_count']\n",
    "\n",
    "svle = pd.merge(svle, courses, on=['code_module', 'code_presentation'], how='left')\n",
    "svle['avg click/day'] =  svle['total_sum_click'] / svle['module_presentation_length']\n",
    "\n",
    "svle = svle.rename(columns={'unique_date_count': 'days_logged', 'unique_id_site_count': 'material_interactions', 'module_presentation_length': 'module_length'})\n",
    "\n",
    "vle_n = vle.groupby(['code_module', 'code_presentation'])['id_site'].nunique().reset_index()\n",
    "svle = pd.merge(svle, vle_n, on=['code_module', 'code_presentation'], how='left')\n",
    "svle['% material interaction'] =  100*svle['material_interactions'] / svle['id_site']\n",
    "\n",
    "assessment_counts = assessments.groupby(['code_module', 'code_presentation'])['assessment_type'].value_counts().reset_index(name='count')\n",
    "assessment_pivot = assessment_counts.pivot_table(index=['code_module', 'code_presentation'], columns='assessment_type', values='count', fill_value=0).reset_index()\n",
    "assessment_pivot.columns.name = None  \n",
    "assessment_pivot.columns = ['code_module', 'code_presentation', 'CMA', 'Exam', 'TMA']\n",
    "svle = pd.merge(svle, assessment_pivot, on=['code_module', 'code_presentation'], how='left')\n",
    "\n",
    "df_all = pd.merge(svle, student_df, on=['code_module', 'code_presentation', 'id_student'], how='inner')\n",
    "columns_to_move = ['final_result', 'score', 'id_student']\n",
    "df_all = df_all[columns_to_move + [col for col in df_all.columns if col not in columns_to_move]]\n",
    "\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13169, 27)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop withdrawn students\n",
    "df_all = df_all[df_all['final_result'] != 'Withdrawn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13169, 47)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#order highest_education, imd_band, age_band, disability, studied_credits_binned, final_result\n",
    "highest_education = {\n",
    "    'No Formal quals': 0,\n",
    "    'Lower Than A Level': 1,\n",
    "    'A Level or Equivalent': 2,\n",
    "    'HE Qualification': 3,\n",
    "    'Post Graduate Qualification': 4\n",
    "}\n",
    "\n",
    "imd_band = {\n",
    "    np.nan: -1,\n",
    "    '0-10%': 0,\n",
    "    '10-20': 1,\n",
    "    '20-30%': 2,\n",
    "    '30-40%': 3,\n",
    "    '40-50%': 4,\n",
    "    '50-60%': 5,\n",
    "    '60-70%': 6,\n",
    "    '70-80%': 7,\n",
    "    '80-90%': 8,\n",
    "    '90-100%': 9\n",
    "}\n",
    "\n",
    "age_band = {\n",
    "    '0-35': 0,\n",
    "    '35-55': 1,\n",
    "    '55<=': 2\n",
    "}\n",
    "\n",
    "disability = {\n",
    "    'N': 0,\n",
    "    'Y': 1\n",
    "}\n",
    "\n",
    "studied_credits_binned = {\n",
    "    '30-60': 0,\n",
    "    '61-100': 1,\n",
    "    '101-200': 2,\n",
    "    '201+': 3\n",
    "}\n",
    "\n",
    "final_result = {\n",
    "    'Fail': 0,\n",
    "    'Pass': 1,\n",
    "    'Distinction': 1,\n",
    "    'Withdrawn': 0\n",
    "}\n",
    "\n",
    "data_dummies = pd.get_dummies(df_all, columns=['code_module', 'code_presentation', 'gender', 'region'])\n",
    "data_dummies['highest_education'] = data_dummies['highest_education'].map(highest_education)\n",
    "data_dummies['imd_band'] = data_dummies['imd_band'].map(imd_band)\n",
    "data_dummies['age_band'] = data_dummies['age_band'].map(age_band)\n",
    "data_dummies['disability'] = data_dummies['disability'].map(disability)\n",
    "data_dummies['studied_credits_binned'] = data_dummies['studied_credits_binned'].map(studied_credits_binned)\n",
    "data_dummies['final_result'] = data_dummies['final_result'].map(final_result)\n",
    "data_dummies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummies = data_dummies[~((data_dummies['score'] > 50) & (data_dummies['final_result'] == 0))]\n",
    "data_dummies = data_dummies[~((data_dummies['score'] < 50) & (data_dummies['final_result'] == 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10737, 47)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dummies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_OneToOneFeatureMixin' from 'sklearn.base' (/opt/conda/lib/python3.8/site-packages/sklearn/base.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/imblearn/base.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# scikit-learn >= 1.2\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneToOneFeatureMixin\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'OneToOneFeatureMixin' from 'sklearn.base' (/opt/conda/lib/python3.8/site-packages/sklearn/base.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\u001b[38;5;66;03m#pip install imbalanced-learn\u001b[39;00m\n\u001b[1;32m      7\u001b[0m X_dummy \u001b[38;5;241m=\u001b[39m data_dummies\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_result\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstudied_credits\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_student\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrade\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m X_dummy_combined \u001b[38;5;241m=\u001b[39m X_dummy\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/imblearn/__init__.py:52\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m         combine,\n\u001b[1;32m     54\u001b[0m         ensemble,\n\u001b[1;32m     55\u001b[0m         exceptions,\n\u001b[1;32m     56\u001b[0m         metrics,\n\u001b[1;32m     57\u001b[0m         over_sampling,\n\u001b[1;32m     58\u001b[0m         pipeline,\n\u001b[1;32m     59\u001b[0m         tensorflow,\n\u001b[1;32m     60\u001b[0m         under_sampling,\n\u001b[1;32m     61\u001b[0m         utils,\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/imblearn/combine/__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/imblearn/combine/_smote_enn.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/imblearn/base.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneToOneFeatureMixin\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _OneToOneFeatureMixin \u001b[38;5;28;01mas\u001b[39;00m OneToOneFeatureMixin\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m label_binarize\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_version\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_OneToOneFeatureMixin' from 'sklearn.base' (/opt/conda/lib/python3.8/site-packages/sklearn/base.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE#pip install imbalanced-learn\n",
    "\n",
    "\n",
    "\n",
    "X_dummy = data_dummies.drop(['final_result', 'studied_credits', 'id_student', 'score', 'grade'], axis=1)\n",
    "X_dummy_combined = X_dummy.copy()\n",
    "X_dummy_combined['score'] = data_dummies['score']\n",
    "# X_dummy_combined.loc[(X_dummy_combined['score'] < 50) & (y_dummy['final_result'] == 1), 'score'] = 76\n",
    "# X_dummy_combined.loc[y_dummy['final_result'] == 0, 'score'] = 35\n",
    "\n",
    "\n",
    "y_dummy = data_dummies[['final_result']]\n",
    "\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "X_dummy_smote, y_dummy_smote = smote.fit_resample(X_dummy_combined, y_dummy)\n",
    "\n",
    "y_dummy_smote['score'] = X_dummy_smote['score']\n",
    "y_dummy['score'] = X_dummy_combined['score']\n",
    "X_dummy_smote = X_dummy_smote.drop(['score'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_dummy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43my_dummy\u001b[49m\u001b[38;5;241m.\u001b[39mvalue_counts())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_dummy' is not defined"
     ]
    }
   ],
   "source": [
    "print(y_dummy.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18934, 42)\n",
      "(18934, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    9467\n",
       "0    9467\n",
       "Name: final_result, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_dummy_smote.shape)\n",
    "print(y_dummy_smote.shape)\n",
    "y_dummy_smote['final_result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18934 entries, 0 to 18933\n",
      "Data columns (total 42 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   mean_sum_click               18934 non-null  float64\n",
      " 1   total_sum_click              18934 non-null  float64\n",
      " 2   days_logged                  18934 non-null  int64  \n",
      " 3   material_interactions        18934 non-null  int64  \n",
      " 4   module_length                18934 non-null  int64  \n",
      " 5   avg click/day                18934 non-null  float64\n",
      " 6   id_site                      18934 non-null  int64  \n",
      " 7   % material interaction       18934 non-null  float64\n",
      " 8   CMA                          18934 non-null  int64  \n",
      " 9   Exam                         18934 non-null  int64  \n",
      " 10  TMA                          18934 non-null  int64  \n",
      " 11  highest_education            18934 non-null  int64  \n",
      " 12  imd_band                     18934 non-null  int64  \n",
      " 13  age_band                     18934 non-null  int64  \n",
      " 14  num_of_prev_attempts         18934 non-null  int64  \n",
      " 15  disability                   18934 non-null  int64  \n",
      " 16  studied_credits_binned       18934 non-null  int64  \n",
      " 17  date_registration            18934 non-null  float64\n",
      " 18  code_module_AAA              18934 non-null  uint8  \n",
      " 19  code_module_BBB              18934 non-null  uint8  \n",
      " 20  code_module_CCC              18934 non-null  uint8  \n",
      " 21  code_module_DDD              18934 non-null  uint8  \n",
      " 22  code_module_EEE              18934 non-null  uint8  \n",
      " 23  code_presentation_2013B      18934 non-null  uint8  \n",
      " 24  code_presentation_2013J      18934 non-null  uint8  \n",
      " 25  code_presentation_2014B      18934 non-null  uint8  \n",
      " 26  code_presentation_2014J      18934 non-null  uint8  \n",
      " 27  gender_F                     18934 non-null  uint8  \n",
      " 28  gender_M                     18934 non-null  uint8  \n",
      " 29  region_East Anglian Region   18934 non-null  uint8  \n",
      " 30  region_East Midlands Region  18934 non-null  uint8  \n",
      " 31  region_Ireland               18934 non-null  uint8  \n",
      " 32  region_London Region         18934 non-null  uint8  \n",
      " 33  region_North Region          18934 non-null  uint8  \n",
      " 34  region_North Western Region  18934 non-null  uint8  \n",
      " 35  region_Scotland              18934 non-null  uint8  \n",
      " 36  region_South East Region     18934 non-null  uint8  \n",
      " 37  region_South Region          18934 non-null  uint8  \n",
      " 38  region_South West Region     18934 non-null  uint8  \n",
      " 39  region_Wales                 18934 non-null  uint8  \n",
      " 40  region_West Midlands Region  18934 non-null  uint8  \n",
      " 41  region_Yorkshire Region      18934 non-null  uint8  \n",
      "dtypes: float64(5), int64(13), uint8(24)\n",
      "memory usage: 3.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(X_dummy_smote.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dummy, X_test_dummy, y_train_dummy, y_test_dummy = train_test_split(X_dummy_smote, y_dummy_smote, test_size=0.2, random_state=42, stratify=y_dummy_smote['final_result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Logistic Regression: 0.9308159493002377\n",
      "Confusion Matrix:\n",
      "[[1695  199]\n",
      " [  63 1830]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(max_iter=20000)\n",
    "logreg.fit(X_train_dummy, y_train_dummy['final_result'])\n",
    "\n",
    "y_pred = logreg.predict(X_test_dummy)\n",
    "acc = accuracy_score(y_test_dummy['final_result'], y_pred)\n",
    "print('Accuracy Logistic Regression:', acc)\n",
    "\n",
    "cm = confusion_matrix(y_test_dummy['final_result'], y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "from xgboost import XGBClassifier #pip install xgboost\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# Define the parameter space\n",
    "param_space = {\n",
    "    'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "    'min_child_weight': (1, 10),\n",
    "    'max_depth': (3, 50),\n",
    "    'max_delta_step': (1, 20),\n",
    "    'subsample': (0.01, 1.0, 'uniform'),\n",
    "    'colsample_bytree': (0.01, 1.0, 'uniform'),\n",
    "    'colsample_bylevel': (0.01, 1.0, 'uniform'),\n",
    "    'reg_lambda': (1e-9, 1000, 'log-uniform'),\n",
    "    'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n",
    "    'gamma': (1e-9, 0.5, 'log-uniform'),\n",
    "    'n_estimators': (50, 200),\n",
    "    'scale_pos_weight': (1e-6, 500, 'log-uniform')\n",
    "}\n",
    "\n",
    "# Create a BayesSearchCV object\n",
    "opt_xgb = BayesSearchCV(\n",
    "    estimator=XGBClassifier(n_jobs=-1),\n",
    "    search_spaces=param_space,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    n_iter=150,\n",
    "    verbose=1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# Run the optimization\n",
    "opt_xgb.fit(X_train_dummy, y_train_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy XGBoost: 0.9384737259044098\n",
      "Confusion Matrix:\n",
      "[[1738  156]\n",
      " [  77 1816]]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier #pip install xgboost\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train_dummy, y_train_dummy['final_result'])\n",
    "\n",
    "y_pred = xgb.predict(X_test_dummy)\n",
    "acc = accuracy_score(y_test_dummy['final_result'], y_pred)\n",
    "print(\"Accuracy XGBoost:\", acc)\n",
    "\n",
    "cm = confusion_matrix(y_test_dummy['final_result'], y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 100\n",
      "Completed iteration 1\n",
      "Completed iteration 2\n",
      "Completed iteration 3\n",
      "Completed iteration 4\n",
      "Completed iteration 5\n",
      "Completed iteration 6\n",
      "Completed iteration 7\n",
      "Completed iteration 8\n",
      "Completed iteration 9\n",
      "Completed iteration 10\n",
      "Completed iteration 11\n",
      "Completed iteration 12\n",
      "Completed iteration 13\n",
      "Completed iteration 14\n",
      "Completed iteration 15\n",
      "Completed iteration 16\n",
      "Completed iteration 17\n",
      "Completed iteration 18\n",
      "Completed iteration 19\n",
      "Completed iteration 20\n",
      "Completed iteration 21\n",
      "Completed iteration 22\n",
      "Completed iteration 23\n",
      "Completed iteration 24\n",
      "Completed iteration 25\n",
      "Completed iteration 26\n",
      "Completed iteration 27\n",
      "Completed iteration 28\n",
      "Completed iteration 29\n",
      "Completed iteration 30\n",
      "Completed iteration 31\n",
      "Completed iteration 32\n",
      "Completed iteration 33\n",
      "Completed iteration 34\n",
      "Completed iteration 35\n",
      "Completed iteration 36\n",
      "Completed iteration 37\n",
      "Completed iteration 38\n",
      "Completed iteration 42\n",
      "Completed iteration 44\n",
      "Completed iteration 45\n",
      "Completed iteration 46\n",
      "Completed iteration 47\n",
      "Completed iteration 48\n",
      "Completed iteration 49\n",
      "Completed iteration 50\n",
      "Completed iteration 51\n",
      "Completed iteration 52\n",
      "Completed iteration 53\n",
      "Completed iteration 54\n",
      "Completed iteration 55\n",
      "Completed iteration 56\n",
      "Completed iteration 57\n",
      "Completed iteration 58\n",
      "Completed iteration 59\n",
      "Completed iteration 60\n",
      "Completed iteration 61\n",
      "Completed iteration 62\n",
      "Completed iteration 63\n",
      "Completed iteration 64\n",
      "Completed iteration 65\n",
      "Completed iteration 66\n",
      "Completed iteration 67\n",
      "Completed iteration 68\n",
      "Completed iteration 69\n",
      "Completed iteration 70\n",
      "Completed iteration 71\n",
      "Completed iteration 72\n",
      "Completed iteration 73\n",
      "Completed iteration 74\n",
      "Completed iteration 75\n",
      "Completed iteration 76\n",
      "Completed iteration 77\n",
      "Completed iteration 78\n",
      "Completed iteration 79\n",
      "Completed iteration 80\n",
      "Completed iteration 81\n",
      "Completed iteration 82\n",
      "Completed iteration 83\n",
      "Completed iteration 84\n",
      "Completed iteration 85\n",
      "Completed iteration 86\n",
      "Completed iteration 87\n",
      "Completed iteration 88\n",
      "Completed iteration 89\n",
      "Completed iteration 90\n",
      "Completed iteration 91\n",
      "Completed iteration 92\n",
      "Completed iteration 93\n",
      "Completed iteration 94\n",
      "Completed iteration 95\n",
      "Completed iteration 96\n",
      "Completed iteration 97\n",
      "Completed iteration 98\n",
      "Completed iteration 99\n",
      "Completed iteration 100\n",
      "Completed iteration 101\n",
      "Completed iteration 102\n",
      "Completed iteration 103\n",
      "Completed iteration 104\n",
      "Completed iteration 105\n",
      "Completed iteration 106\n",
      "Completed iteration 107\n",
      "Completed iteration 108\n",
      "Completed iteration 109\n",
      "Completed iteration 110\n",
      "Completed iteration 111\n",
      "Completed iteration 112\n",
      "Completed iteration 113\n",
      "Completed iteration 114\n",
      "Completed iteration 115\n",
      "Completed iteration 116\n",
      "Completed iteration 117\n",
      "Completed iteration 118\n",
      "Completed iteration 119\n",
      "Completed iteration 120\n",
      "Completed iteration 121\n",
      "Completed iteration 122\n",
      "Completed iteration 123\n",
      "Completed iteration 124\n",
      "Completed iteration 125\n",
      "Completed iteration 126\n",
      "Completed iteration 127\n",
      "Completed iteration 128\n",
      "Completed iteration 129\n",
      "Completed iteration 130\n",
      "Completed iteration 131\n",
      "Completed iteration 132\n",
      "Completed iteration 133\n",
      "Completed iteration 134\n",
      "Completed iteration 135\n",
      "Completed iteration 136\n",
      "Completed iteration 137\n",
      "Completed iteration 138\n",
      "Completed iteration 139\n",
      "Completed iteration 140\n",
      "Completed iteration 141\n",
      "Completed iteration 142\n",
      "Completed iteration 143\n",
      "Completed iteration 144\n",
      "Completed iteration 145\n",
      "Completed iteration 146\n",
      "Completed iteration 147\n",
      "Completed iteration 148\n",
      "Completed iteration 149\n",
      "Completed iteration 150\n",
      "Best parameters: OrderedDict([('bagging_temperature', 1.0), ('border_count', 86), ('depth', 16), ('iterations', 868), ('l2_leaf_reg', 2), ('learning_rate', 0.06598200301644333), ('random_strength', 10.0), ('scale_pos_weight', 0.8384915263866678)])\n",
      "Best score: 0.945071830392019\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "counter = [0]\n",
    "\n",
    "def on_step(optim_result):\n",
    "    # Increment the counter\n",
    "    counter[0] += 1\n",
    "    print(f\"Completed iteration {counter[0]}\")\n",
    "    \n",
    "cb = CatBoostClassifier(verbose=False)\n",
    "\n",
    "# Define search spaces\n",
    "param_space = {\n",
    "    'iterations': (50, 1000),\n",
    "    'depth': (1, 16),\n",
    "    'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "    'random_strength': (1e-9, 10, 'log-uniform'),\n",
    "    'bagging_temperature': (0.0, 1.0),\n",
    "    'border_count': (1, 255),\n",
    "    'l2_leaf_reg': (2, 30),\n",
    "    'scale_pos_weight':(0.01, 1.0, 'uniform')\n",
    "}\n",
    "\n",
    "# Initialize BayesSearchCV\n",
    "opt_cb = BayesSearchCV(\n",
    "    estimator=cb,\n",
    "    search_spaces=param_space,\n",
    "    scoring='accuracy',\n",
    "    n_iter=150,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=False,\n",
    "    refit=True,\n",
    "    optimizer_kwargs={'base_estimator': 'GP'},\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "opt_cb.fit(X_train_dummy, y_train_dummy['final_result'], callback=on_step)\n",
    "\n",
    "# Print best parameters and score\n",
    "print(f'Best parameters: {opt_cb.best_params_}')\n",
    "print(f'Best score: {opt_cb.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6405007\ttotal: 831ms\tremaining: 13m 49s\n",
      "1:\tlearn: 0.6020708\ttotal: 1.5s\tremaining: 12m 27s\n",
      "2:\tlearn: 0.5471311\ttotal: 2.19s\tremaining: 12m 6s\n",
      "3:\tlearn: 0.4990458\ttotal: 2.6s\tremaining: 10m 47s\n",
      "4:\tlearn: 0.4897704\ttotal: 2.61s\tremaining: 8m 39s\n",
      "5:\tlearn: 0.4649490\ttotal: 3.34s\tremaining: 9m 13s\n",
      "6:\tlearn: 0.4397404\ttotal: 3.98s\tremaining: 9m 25s\n",
      "7:\tlearn: 0.4239931\ttotal: 4.6s\tremaining: 9m 30s\n",
      "8:\tlearn: 0.4055678\ttotal: 5.22s\tremaining: 9m 35s\n",
      "9:\tlearn: 0.3950296\ttotal: 5.27s\tremaining: 8m 41s\n",
      "10:\tlearn: 0.3753057\ttotal: 5.9s\tremaining: 8m 50s\n",
      "11:\tlearn: 0.3535420\ttotal: 6.29s\tremaining: 8m 38s\n",
      "12:\tlearn: 0.3434146\ttotal: 6.95s\tremaining: 8m 47s\n",
      "13:\tlearn: 0.3267107\ttotal: 7.6s\tremaining: 8m 55s\n",
      "14:\tlearn: 0.3146954\ttotal: 8.26s\tremaining: 9m 2s\n",
      "15:\tlearn: 0.3063732\ttotal: 8.35s\tremaining: 8m 33s\n",
      "16:\tlearn: 0.2992469\ttotal: 8.51s\tremaining: 8m 12s\n",
      "17:\tlearn: 0.2905460\ttotal: 9.17s\tremaining: 8m 20s\n",
      "18:\tlearn: 0.2803186\ttotal: 9.82s\tremaining: 8m 26s\n",
      "19:\tlearn: 0.2698726\ttotal: 10.5s\tremaining: 8m 33s\n",
      "20:\tlearn: 0.2632820\ttotal: 11.2s\tremaining: 8m 39s\n",
      "21:\tlearn: 0.2549250\ttotal: 11.2s\tremaining: 8m 19s\n",
      "22:\tlearn: 0.2465786\ttotal: 11.9s\tremaining: 8m 25s\n",
      "23:\tlearn: 0.2433338\ttotal: 12s\tremaining: 8m 7s\n",
      "24:\tlearn: 0.2354575\ttotal: 12.4s\tremaining: 8m 2s\n",
      "25:\tlearn: 0.2329254\ttotal: 12.5s\tremaining: 7m 47s\n",
      "26:\tlearn: 0.2275666\ttotal: 12.5s\tremaining: 7m 29s\n",
      "27:\tlearn: 0.2255341\ttotal: 12.5s\tremaining: 7m 13s\n",
      "28:\tlearn: 0.2250290\ttotal: 12.5s\tremaining: 6m 58s\n",
      "29:\tlearn: 0.2188385\ttotal: 13.1s\tremaining: 7m 4s\n",
      "30:\tlearn: 0.2151218\ttotal: 13.2s\tremaining: 6m 52s\n",
      "31:\tlearn: 0.2112176\ttotal: 13.8s\tremaining: 6m 58s\n",
      "32:\tlearn: 0.2055237\ttotal: 14.5s\tremaining: 7m 5s\n",
      "33:\tlearn: 0.2006389\ttotal: 15.2s\tremaining: 7m 10s\n",
      "34:\tlearn: 0.1968746\ttotal: 15.8s\tremaining: 7m 16s\n",
      "35:\tlearn: 0.1931997\ttotal: 16.5s\tremaining: 7m 21s\n",
      "36:\tlearn: 0.1906573\ttotal: 17.1s\tremaining: 7m 26s\n",
      "37:\tlearn: 0.1864181\ttotal: 17.8s\tremaining: 7m 30s\n",
      "38:\tlearn: 0.1839363\ttotal: 18.5s\tremaining: 7m 35s\n",
      "39:\tlearn: 0.1815242\ttotal: 19.1s\tremaining: 7m 38s\n",
      "40:\tlearn: 0.1783868\ttotal: 19.8s\tremaining: 7m 43s\n",
      "41:\tlearn: 0.1742096\ttotal: 20.5s\tremaining: 7m 46s\n",
      "42:\tlearn: 0.1709949\ttotal: 21.1s\tremaining: 7m 50s\n",
      "43:\tlearn: 0.1701193\ttotal: 21.1s\tremaining: 7m 39s\n",
      "44:\tlearn: 0.1675779\ttotal: 21.3s\tremaining: 7m 31s\n",
      "45:\tlearn: 0.1663592\ttotal: 21.9s\tremaining: 7m 34s\n",
      "46:\tlearn: 0.1646384\ttotal: 22.6s\tremaining: 7m 37s\n",
      "47:\tlearn: 0.1628539\ttotal: 23.2s\tremaining: 7m 40s\n",
      "48:\tlearn: 0.1603668\ttotal: 23.9s\tremaining: 7m 43s\n",
      "49:\tlearn: 0.1577574\ttotal: 24.5s\tremaining: 7m 45s\n",
      "50:\tlearn: 0.1569804\ttotal: 24.5s\tremaining: 7m 36s\n",
      "51:\tlearn: 0.1548113\ttotal: 25.2s\tremaining: 7m 38s\n",
      "52:\tlearn: 0.1520639\ttotal: 25.9s\tremaining: 7m 42s\n",
      "53:\tlearn: 0.1493488\ttotal: 26.6s\tremaining: 7m 45s\n",
      "54:\tlearn: 0.1467481\ttotal: 27.3s\tremaining: 7m 48s\n",
      "55:\tlearn: 0.1451223\ttotal: 28s\tremaining: 7m 52s\n",
      "56:\tlearn: 0.1434344\ttotal: 28.6s\tremaining: 7m 53s\n",
      "57:\tlearn: 0.1434230\ttotal: 28.7s\tremaining: 7m 45s\n",
      "58:\tlearn: 0.1407205\ttotal: 29.3s\tremaining: 7m 47s\n",
      "59:\tlearn: 0.1407156\ttotal: 29.3s\tremaining: 7m 39s\n",
      "60:\tlearn: 0.1396570\ttotal: 29.4s\tremaining: 7m 32s\n",
      "61:\tlearn: 0.1389398\ttotal: 29.4s\tremaining: 7m 25s\n",
      "62:\tlearn: 0.1369969\ttotal: 30.1s\tremaining: 7m 27s\n",
      "63:\tlearn: 0.1344655\ttotal: 30.7s\tremaining: 7m 28s\n",
      "64:\tlearn: 0.1341862\ttotal: 30.7s\tremaining: 7m 21s\n",
      "65:\tlearn: 0.1327065\ttotal: 31.3s\tremaining: 7m 23s\n",
      "66:\tlearn: 0.1303445\ttotal: 32s\tremaining: 7m 25s\n",
      "67:\tlearn: 0.1282640\ttotal: 32.7s\tremaining: 7m 28s\n",
      "68:\tlearn: 0.1280241\ttotal: 32.7s\tremaining: 7m 21s\n",
      "69:\tlearn: 0.1279316\ttotal: 32.7s\tremaining: 7m 14s\n",
      "70:\tlearn: 0.1259982\ttotal: 33.4s\tremaining: 7m 17s\n",
      "71:\tlearn: 0.1235835\ttotal: 34s\tremaining: 7m 18s\n",
      "72:\tlearn: 0.1233096\ttotal: 34s\tremaining: 7m 12s\n",
      "73:\tlearn: 0.1216196\ttotal: 34.7s\tremaining: 7m 13s\n",
      "74:\tlearn: 0.1202449\ttotal: 35.3s\tremaining: 7m 15s\n",
      "75:\tlearn: 0.1202068\ttotal: 35.3s\tremaining: 7m 9s\n",
      "76:\tlearn: 0.1179490\ttotal: 35.9s\tremaining: 7m 10s\n",
      "77:\tlearn: 0.1173519\ttotal: 36s\tremaining: 7m 4s\n",
      "78:\tlearn: 0.1163154\ttotal: 36s\tremaining: 6m 59s\n",
      "79:\tlearn: 0.1146446\ttotal: 36.6s\tremaining: 7m\n",
      "80:\tlearn: 0.1134916\ttotal: 36.7s\tremaining: 6m 56s\n",
      "81:\tlearn: 0.1119134\ttotal: 37.3s\tremaining: 6m 57s\n",
      "82:\tlearn: 0.1104093\ttotal: 38s\tremaining: 7m\n",
      "83:\tlearn: 0.1094347\ttotal: 38.2s\tremaining: 6m 56s\n",
      "84:\tlearn: 0.1082804\ttotal: 38.9s\tremaining: 6m 59s\n",
      "85:\tlearn: 0.1066724\ttotal: 39.6s\tremaining: 7m\n",
      "86:\tlearn: 0.1066705\ttotal: 39.6s\tremaining: 6m 55s\n",
      "87:\tlearn: 0.1056887\ttotal: 40.2s\tremaining: 6m 56s\n",
      "88:\tlearn: 0.1050235\ttotal: 40.2s\tremaining: 6m 51s\n",
      "89:\tlearn: 0.1047592\ttotal: 40.2s\tremaining: 6m 46s\n",
      "90:\tlearn: 0.1035526\ttotal: 40.9s\tremaining: 6m 48s\n",
      "91:\tlearn: 0.1025069\ttotal: 41.5s\tremaining: 6m 49s\n",
      "92:\tlearn: 0.1013489\ttotal: 42.1s\tremaining: 6m 50s\n",
      "93:\tlearn: 0.1004812\ttotal: 42.2s\tremaining: 6m 46s\n",
      "94:\tlearn: 0.0991817\ttotal: 42.8s\tremaining: 6m 47s\n",
      "95:\tlearn: 0.0976770\ttotal: 43.5s\tremaining: 6m 49s\n",
      "96:\tlearn: 0.0964674\ttotal: 44.1s\tremaining: 6m 50s\n",
      "97:\tlearn: 0.0956948\ttotal: 44.2s\tremaining: 6m 47s\n",
      "98:\tlearn: 0.0954514\ttotal: 44.3s\tremaining: 6m 42s\n",
      "99:\tlearn: 0.0946856\ttotal: 44.9s\tremaining: 6m 43s\n",
      "100:\tlearn: 0.0939856\ttotal: 44.9s\tremaining: 6m 39s\n",
      "101:\tlearn: 0.0939098\ttotal: 44.9s\tremaining: 6m 35s\n",
      "102:\tlearn: 0.0927583\ttotal: 45.5s\tremaining: 6m 36s\n",
      "103:\tlearn: 0.0920935\ttotal: 46.2s\tremaining: 6m 37s\n",
      "104:\tlearn: 0.0920065\ttotal: 46.2s\tremaining: 6m 33s\n",
      "105:\tlearn: 0.0910442\ttotal: 46.8s\tremaining: 6m 34s\n",
      "106:\tlearn: 0.0909088\ttotal: 46.8s\tremaining: 6m 30s\n",
      "107:\tlearn: 0.0895568\ttotal: 47.4s\tremaining: 6m 31s\n",
      "108:\tlearn: 0.0887235\ttotal: 47.6s\tremaining: 6m 28s\n",
      "109:\tlearn: 0.0883804\ttotal: 47.6s\tremaining: 6m 25s\n",
      "110:\tlearn: 0.0877884\ttotal: 48.2s\tremaining: 6m 26s\n",
      "111:\tlearn: 0.0866153\ttotal: 48.9s\tremaining: 6m 27s\n",
      "112:\tlearn: 0.0856391\ttotal: 49.5s\tremaining: 6m 28s\n",
      "113:\tlearn: 0.0849915\ttotal: 49.5s\tremaining: 6m 25s\n",
      "114:\tlearn: 0.0847232\ttotal: 49.6s\tremaining: 6m 21s\n",
      "115:\tlearn: 0.0842506\ttotal: 50.2s\tremaining: 6m 22s\n",
      "116:\tlearn: 0.0833749\ttotal: 50.8s\tremaining: 6m 23s\n",
      "117:\tlearn: 0.0823483\ttotal: 51.5s\tremaining: 6m 24s\n",
      "118:\tlearn: 0.0813816\ttotal: 51.9s\tremaining: 6m 23s\n",
      "119:\tlearn: 0.0812365\ttotal: 51.9s\tremaining: 6m 20s\n",
      "120:\tlearn: 0.0803236\ttotal: 52.5s\tremaining: 6m 21s\n",
      "121:\tlearn: 0.0797678\ttotal: 53.1s\tremaining: 6m 22s\n",
      "122:\tlearn: 0.0789830\ttotal: 53.8s\tremaining: 6m 23s\n",
      "123:\tlearn: 0.0787460\ttotal: 53.8s\tremaining: 6m 20s\n",
      "124:\tlearn: 0.0787109\ttotal: 53.8s\tremaining: 6m 16s\n",
      "125:\tlearn: 0.0785456\ttotal: 53.8s\tremaining: 6m 13s\n",
      "126:\tlearn: 0.0777912\ttotal: 54.4s\tremaining: 6m 14s\n",
      "127:\tlearn: 0.0777481\ttotal: 54.5s\tremaining: 6m 10s\n",
      "128:\tlearn: 0.0768209\ttotal: 55.1s\tremaining: 6m 12s\n",
      "129:\tlearn: 0.0762120\ttotal: 55.1s\tremaining: 6m 9s\n",
      "130:\tlearn: 0.0754677\ttotal: 55.8s\tremaining: 6m 10s\n",
      "131:\tlearn: 0.0746451\ttotal: 56.2s\tremaining: 6m 9s\n",
      "132:\tlearn: 0.0745501\ttotal: 56.2s\tremaining: 6m 6s\n",
      "133:\tlearn: 0.0736551\ttotal: 56.8s\tremaining: 6m 7s\n",
      "134:\tlearn: 0.0732891\ttotal: 56.9s\tremaining: 6m 4s\n",
      "135:\tlearn: 0.0722259\ttotal: 57.5s\tremaining: 6m 5s\n",
      "136:\tlearn: 0.0720382\ttotal: 57.5s\tremaining: 6m 2s\n",
      "137:\tlearn: 0.0714284\ttotal: 57.5s\tremaining: 5m 59s\n",
      "138:\tlearn: 0.0704678\ttotal: 58.2s\tremaining: 6m\n",
      "139:\tlearn: 0.0702616\ttotal: 58.2s\tremaining: 5m 57s\n",
      "140:\tlearn: 0.0696050\ttotal: 58.8s\tremaining: 5m 58s\n",
      "141:\tlearn: 0.0690131\ttotal: 59.5s\tremaining: 5m 59s\n",
      "142:\tlearn: 0.0683177\ttotal: 1m\tremaining: 6m\n",
      "143:\tlearn: 0.0681307\ttotal: 1m\tremaining: 5m 57s\n",
      "144:\tlearn: 0.0679930\ttotal: 1m\tremaining: 5m 54s\n",
      "145:\tlearn: 0.0676241\ttotal: 1m\tremaining: 5m 55s\n",
      "146:\tlearn: 0.0668470\ttotal: 1m 1s\tremaining: 5m 56s\n",
      "147:\tlearn: 0.0663254\ttotal: 1m 2s\tremaining: 5m 57s\n",
      "148:\tlearn: 0.0662216\ttotal: 1m 2s\tremaining: 5m 54s\n",
      "149:\tlearn: 0.0654505\ttotal: 1m 2s\tremaining: 5m 55s\n",
      "150:\tlearn: 0.0644860\ttotal: 1m 3s\tremaining: 5m 56s\n",
      "151:\tlearn: 0.0637968\ttotal: 1m 4s\tremaining: 5m 57s\n",
      "152:\tlearn: 0.0629490\ttotal: 1m 4s\tremaining: 5m 58s\n",
      "153:\tlearn: 0.0628136\ttotal: 1m 4s\tremaining: 5m 56s\n",
      "154:\tlearn: 0.0623117\ttotal: 1m 5s\tremaining: 5m 56s\n",
      "155:\tlearn: 0.0614844\ttotal: 1m 6s\tremaining: 5m 57s\n",
      "156:\tlearn: 0.0609455\ttotal: 1m 6s\tremaining: 5m 58s\n",
      "157:\tlearn: 0.0603451\ttotal: 1m 7s\tremaining: 6m\n",
      "158:\tlearn: 0.0591992\ttotal: 1m 8s\tremaining: 6m\n",
      "159:\tlearn: 0.0584910\ttotal: 1m 8s\tremaining: 6m 1s\n",
      "160:\tlearn: 0.0576594\ttotal: 1m 9s\tremaining: 6m 2s\n",
      "161:\tlearn: 0.0568787\ttotal: 1m 10s\tremaining: 6m 2s\n",
      "162:\tlearn: 0.0562978\ttotal: 1m 10s\tremaining: 6m 3s\n",
      "163:\tlearn: 0.0562426\ttotal: 1m 10s\tremaining: 6m 1s\n",
      "164:\tlearn: 0.0557073\ttotal: 1m 11s\tremaining: 6m 1s\n",
      "165:\tlearn: 0.0548304\ttotal: 1m 12s\tremaining: 6m 2s\n",
      "166:\tlearn: 0.0538506\ttotal: 1m 12s\tremaining: 6m 3s\n",
      "167:\tlearn: 0.0530792\ttotal: 1m 13s\tremaining: 6m 3s\n",
      "168:\tlearn: 0.0523446\ttotal: 1m 14s\tremaining: 6m 4s\n",
      "169:\tlearn: 0.0517087\ttotal: 1m 14s\tremaining: 6m 4s\n",
      "170:\tlearn: 0.0510130\ttotal: 1m 15s\tremaining: 6m 5s\n",
      "171:\tlearn: 0.0500903\ttotal: 1m 16s\tremaining: 6m 5s\n",
      "172:\tlearn: 0.0492938\ttotal: 1m 16s\tremaining: 6m 6s\n",
      "173:\tlearn: 0.0484551\ttotal: 1m 17s\tremaining: 6m 6s\n",
      "174:\tlearn: 0.0477365\ttotal: 1m 17s\tremaining: 6m 7s\n",
      "175:\tlearn: 0.0469203\ttotal: 1m 18s\tremaining: 6m 7s\n",
      "176:\tlearn: 0.0461206\ttotal: 1m 19s\tremaining: 6m 8s\n",
      "177:\tlearn: 0.0455347\ttotal: 1m 19s\tremaining: 6m 8s\n",
      "178:\tlearn: 0.0448986\ttotal: 1m 20s\tremaining: 6m 9s\n",
      "179:\tlearn: 0.0442984\ttotal: 1m 21s\tremaining: 6m 9s\n",
      "180:\tlearn: 0.0434264\ttotal: 1m 21s\tremaining: 6m 9s\n",
      "181:\tlearn: 0.0427117\ttotal: 1m 22s\tremaining: 6m 10s\n",
      "182:\tlearn: 0.0420235\ttotal: 1m 23s\tremaining: 6m 10s\n",
      "183:\tlearn: 0.0413904\ttotal: 1m 23s\tremaining: 6m 11s\n",
      "184:\tlearn: 0.0383272\ttotal: 1m 24s\tremaining: 6m 11s\n",
      "185:\tlearn: 0.0377392\ttotal: 1m 25s\tremaining: 6m 12s\n",
      "186:\tlearn: 0.0372012\ttotal: 1m 25s\tremaining: 6m 12s\n",
      "187:\tlearn: 0.0365497\ttotal: 1m 26s\tremaining: 6m 12s\n",
      "188:\tlearn: 0.0360149\ttotal: 1m 27s\tremaining: 6m 13s\n",
      "189:\tlearn: 0.0354863\ttotal: 1m 27s\tremaining: 6m 13s\n",
      "190:\tlearn: 0.0349221\ttotal: 1m 28s\tremaining: 6m 14s\n",
      "191:\tlearn: 0.0345326\ttotal: 1m 29s\tremaining: 6m 14s\n",
      "192:\tlearn: 0.0337607\ttotal: 1m 29s\tremaining: 6m 14s\n",
      "193:\tlearn: 0.0319631\ttotal: 1m 30s\tremaining: 6m 15s\n",
      "194:\tlearn: 0.0315625\ttotal: 1m 30s\tremaining: 6m 15s\n",
      "195:\tlearn: 0.0311777\ttotal: 1m 31s\tremaining: 6m 15s\n",
      "196:\tlearn: 0.0308020\ttotal: 1m 32s\tremaining: 6m 15s\n",
      "197:\tlearn: 0.0303538\ttotal: 1m 32s\tremaining: 6m 16s\n",
      "198:\tlearn: 0.0299021\ttotal: 1m 33s\tremaining: 6m 16s\n",
      "199:\tlearn: 0.0295528\ttotal: 1m 34s\tremaining: 6m 16s\n",
      "200:\tlearn: 0.0292038\ttotal: 1m 34s\tremaining: 6m 17s\n",
      "201:\tlearn: 0.0288032\ttotal: 1m 35s\tremaining: 6m 17s\n",
      "202:\tlearn: 0.0284604\ttotal: 1m 36s\tremaining: 6m 17s\n",
      "203:\tlearn: 0.0281937\ttotal: 1m 36s\tremaining: 6m 18s\n",
      "204:\tlearn: 0.0279273\ttotal: 1m 37s\tremaining: 6m 18s\n",
      "205:\tlearn: 0.0275596\ttotal: 1m 38s\tremaining: 6m 19s\n",
      "206:\tlearn: 0.0262799\ttotal: 1m 39s\tremaining: 6m 19s\n",
      "207:\tlearn: 0.0260144\ttotal: 1m 39s\tremaining: 6m 19s\n",
      "208:\tlearn: 0.0257028\ttotal: 1m 40s\tremaining: 6m 19s\n",
      "209:\tlearn: 0.0254543\ttotal: 1m 40s\tremaining: 6m 19s\n",
      "210:\tlearn: 0.0251655\ttotal: 1m 41s\tremaining: 6m 19s\n",
      "211:\tlearn: 0.0249250\ttotal: 1m 42s\tremaining: 6m 19s\n",
      "212:\tlearn: 0.0246315\ttotal: 1m 42s\tremaining: 6m 19s\n",
      "213:\tlearn: 0.0243647\ttotal: 1m 43s\tremaining: 6m 20s\n",
      "214:\tlearn: 0.0240676\ttotal: 1m 44s\tremaining: 6m 20s\n",
      "215:\tlearn: 0.0238043\ttotal: 1m 44s\tremaining: 6m 20s\n",
      "216:\tlearn: 0.0234973\ttotal: 1m 45s\tremaining: 6m 20s\n",
      "217:\tlearn: 0.0232326\ttotal: 1m 46s\tremaining: 6m 20s\n",
      "218:\tlearn: 0.0223955\ttotal: 1m 46s\tremaining: 6m 20s\n",
      "219:\tlearn: 0.0220945\ttotal: 1m 47s\tremaining: 6m 20s\n",
      "220:\tlearn: 0.0218560\ttotal: 1m 47s\tremaining: 6m 20s\n",
      "221:\tlearn: 0.0216248\ttotal: 1m 48s\tremaining: 6m 20s\n",
      "222:\tlearn: 0.0214286\ttotal: 1m 49s\tremaining: 6m 20s\n",
      "223:\tlearn: 0.0211894\ttotal: 1m 49s\tremaining: 6m 20s\n",
      "224:\tlearn: 0.0209140\ttotal: 1m 50s\tremaining: 6m 20s\n",
      "225:\tlearn: 0.0207131\ttotal: 1m 51s\tremaining: 6m 20s\n",
      "226:\tlearn: 0.0204785\ttotal: 1m 51s\tremaining: 6m 20s\n",
      "227:\tlearn: 0.0203300\ttotal: 1m 52s\tremaining: 6m 20s\n",
      "228:\tlearn: 0.0201298\ttotal: 1m 53s\tremaining: 6m 20s\n",
      "229:\tlearn: 0.0199179\ttotal: 1m 53s\tremaining: 6m 20s\n",
      "230:\tlearn: 0.0197393\ttotal: 1m 54s\tremaining: 6m 20s\n",
      "231:\tlearn: 0.0195635\ttotal: 1m 54s\tremaining: 6m 20s\n",
      "232:\tlearn: 0.0193824\ttotal: 1m 55s\tremaining: 6m 20s\n",
      "233:\tlearn: 0.0191895\ttotal: 1m 56s\tremaining: 6m 20s\n",
      "234:\tlearn: 0.0189740\ttotal: 1m 56s\tremaining: 6m 20s\n",
      "235:\tlearn: 0.0183182\ttotal: 1m 57s\tremaining: 6m 20s\n",
      "236:\tlearn: 0.0181145\ttotal: 1m 58s\tremaining: 6m 20s\n",
      "237:\tlearn: 0.0179615\ttotal: 1m 58s\tremaining: 6m 20s\n",
      "238:\tlearn: 0.0177588\ttotal: 1m 59s\tremaining: 6m 20s\n",
      "239:\tlearn: 0.0175619\ttotal: 2m\tremaining: 6m 20s\n",
      "240:\tlearn: 0.0173790\ttotal: 2m\tremaining: 6m 20s\n",
      "241:\tlearn: 0.0172058\ttotal: 2m 1s\tremaining: 6m 20s\n",
      "242:\tlearn: 0.0170445\ttotal: 2m 2s\tremaining: 6m 20s\n",
      "243:\tlearn: 0.0168943\ttotal: 2m 2s\tremaining: 6m 20s\n",
      "244:\tlearn: 0.0167565\ttotal: 2m 3s\tremaining: 6m 20s\n",
      "245:\tlearn: 0.0166187\ttotal: 2m 4s\tremaining: 6m 20s\n",
      "246:\tlearn: 0.0164853\ttotal: 2m 4s\tremaining: 6m 20s\n",
      "247:\tlearn: 0.0163594\ttotal: 2m 5s\tremaining: 6m 20s\n",
      "248:\tlearn: 0.0161934\ttotal: 2m 6s\tremaining: 6m 20s\n",
      "249:\tlearn: 0.0160687\ttotal: 2m 6s\tremaining: 6m 20s\n",
      "250:\tlearn: 0.0159407\ttotal: 2m 7s\tremaining: 6m 20s\n",
      "251:\tlearn: 0.0158215\ttotal: 2m 8s\tremaining: 6m 20s\n",
      "252:\tlearn: 0.0157036\ttotal: 2m 8s\tremaining: 6m 20s\n",
      "253:\tlearn: 0.0155773\ttotal: 2m 9s\tremaining: 6m 20s\n",
      "254:\tlearn: 0.0154323\ttotal: 2m 10s\tremaining: 6m 20s\n",
      "255:\tlearn: 0.0153200\ttotal: 2m 10s\tremaining: 6m 20s\n",
      "256:\tlearn: 0.0152048\ttotal: 2m 11s\tremaining: 6m 20s\n",
      "257:\tlearn: 0.0150633\ttotal: 2m 12s\tremaining: 6m 20s\n",
      "258:\tlearn: 0.0149485\ttotal: 2m 12s\tremaining: 6m 19s\n",
      "259:\tlearn: 0.0148209\ttotal: 2m 13s\tremaining: 6m 19s\n",
      "260:\tlearn: 0.0147184\ttotal: 2m 14s\tremaining: 6m 19s\n",
      "261:\tlearn: 0.0146200\ttotal: 2m 14s\tremaining: 6m 19s\n",
      "262:\tlearn: 0.0144786\ttotal: 2m 15s\tremaining: 6m 19s\n",
      "263:\tlearn: 0.0143533\ttotal: 2m 16s\tremaining: 6m 19s\n",
      "264:\tlearn: 0.0142536\ttotal: 2m 16s\tremaining: 6m 19s\n",
      "265:\tlearn: 0.0141301\ttotal: 2m 17s\tremaining: 6m 19s\n",
      "266:\tlearn: 0.0140130\ttotal: 2m 17s\tremaining: 6m 18s\n",
      "267:\tlearn: 0.0139225\ttotal: 2m 18s\tremaining: 6m 18s\n",
      "268:\tlearn: 0.0138225\ttotal: 2m 19s\tremaining: 6m 18s\n",
      "269:\tlearn: 0.0137387\ttotal: 2m 19s\tremaining: 6m 18s\n",
      "270:\tlearn: 0.0136324\ttotal: 2m 20s\tremaining: 6m 18s\n",
      "271:\tlearn: 0.0135524\ttotal: 2m 21s\tremaining: 6m 18s\n",
      "272:\tlearn: 0.0134450\ttotal: 2m 21s\tremaining: 6m 17s\n",
      "273:\tlearn: 0.0133329\ttotal: 2m 22s\tremaining: 6m 17s\n",
      "274:\tlearn: 0.0132291\ttotal: 2m 23s\tremaining: 6m 17s\n",
      "275:\tlearn: 0.0131289\ttotal: 2m 23s\tremaining: 6m 17s\n",
      "276:\tlearn: 0.0130457\ttotal: 2m 24s\tremaining: 6m 17s\n",
      "277:\tlearn: 0.0126652\ttotal: 2m 25s\tremaining: 6m 16s\n",
      "278:\tlearn: 0.0125576\ttotal: 2m 25s\tremaining: 6m 16s\n",
      "279:\tlearn: 0.0124863\ttotal: 2m 26s\tremaining: 6m 16s\n",
      "280:\tlearn: 0.0123839\ttotal: 2m 27s\tremaining: 6m 16s\n",
      "281:\tlearn: 0.0122732\ttotal: 2m 27s\tremaining: 6m 16s\n",
      "282:\tlearn: 0.0121757\ttotal: 2m 28s\tremaining: 6m 16s\n",
      "283:\tlearn: 0.0120675\ttotal: 2m 29s\tremaining: 6m 15s\n",
      "284:\tlearn: 0.0119815\ttotal: 2m 29s\tremaining: 6m 15s\n",
      "285:\tlearn: 0.0118623\ttotal: 2m 30s\tremaining: 6m 15s\n",
      "286:\tlearn: 0.0117606\ttotal: 2m 31s\tremaining: 6m 15s\n",
      "287:\tlearn: 0.0116534\ttotal: 2m 31s\tremaining: 6m 15s\n",
      "288:\tlearn: 0.0115635\ttotal: 2m 32s\tremaining: 6m 14s\n",
      "289:\tlearn: 0.0114826\ttotal: 2m 33s\tremaining: 6m 14s\n",
      "290:\tlearn: 0.0113998\ttotal: 2m 33s\tremaining: 6m 14s\n",
      "291:\tlearn: 0.0113237\ttotal: 2m 34s\tremaining: 6m 14s\n",
      "292:\tlearn: 0.0112365\ttotal: 2m 35s\tremaining: 6m 14s\n",
      "293:\tlearn: 0.0111647\ttotal: 2m 35s\tremaining: 6m 13s\n",
      "294:\tlearn: 0.0110654\ttotal: 2m 36s\tremaining: 6m 13s\n",
      "295:\tlearn: 0.0109911\ttotal: 2m 37s\tremaining: 6m 13s\n",
      "296:\tlearn: 0.0109291\ttotal: 2m 37s\tremaining: 6m 13s\n",
      "297:\tlearn: 0.0108522\ttotal: 2m 38s\tremaining: 6m 13s\n",
      "298:\tlearn: 0.0107888\ttotal: 2m 39s\tremaining: 6m 12s\n",
      "299:\tlearn: 0.0107162\ttotal: 2m 39s\tremaining: 6m 12s\n",
      "300:\tlearn: 0.0103984\ttotal: 2m 40s\tremaining: 6m 12s\n",
      "301:\tlearn: 0.0103312\ttotal: 2m 41s\tremaining: 6m 12s\n",
      "302:\tlearn: 0.0102580\ttotal: 2m 41s\tremaining: 6m 12s\n",
      "303:\tlearn: 0.0101920\ttotal: 2m 42s\tremaining: 6m 11s\n",
      "304:\tlearn: 0.0101246\ttotal: 2m 43s\tremaining: 6m 11s\n",
      "305:\tlearn: 0.0100593\ttotal: 2m 43s\tremaining: 6m 11s\n",
      "306:\tlearn: 0.0099862\ttotal: 2m 44s\tremaining: 6m 10s\n",
      "307:\tlearn: 0.0099243\ttotal: 2m 44s\tremaining: 6m 10s\n",
      "308:\tlearn: 0.0098535\ttotal: 2m 45s\tremaining: 6m 10s\n",
      "309:\tlearn: 0.0097875\ttotal: 2m 46s\tremaining: 6m 10s\n",
      "310:\tlearn: 0.0097141\ttotal: 2m 46s\tremaining: 6m 9s\n",
      "311:\tlearn: 0.0096571\ttotal: 2m 47s\tremaining: 6m 9s\n",
      "312:\tlearn: 0.0095890\ttotal: 2m 48s\tremaining: 6m 9s\n",
      "313:\tlearn: 0.0095341\ttotal: 2m 48s\tremaining: 6m 8s\n",
      "314:\tlearn: 0.0094724\ttotal: 2m 49s\tremaining: 6m 8s\n",
      "315:\tlearn: 0.0094092\ttotal: 2m 50s\tremaining: 6m 8s\n",
      "316:\tlearn: 0.0093485\ttotal: 2m 50s\tremaining: 6m 8s\n",
      "317:\tlearn: 0.0092905\ttotal: 2m 51s\tremaining: 6m 7s\n",
      "318:\tlearn: 0.0092336\ttotal: 2m 52s\tremaining: 6m 7s\n",
      "319:\tlearn: 0.0091708\ttotal: 2m 52s\tremaining: 6m 7s\n",
      "320:\tlearn: 0.0091140\ttotal: 2m 53s\tremaining: 6m 6s\n",
      "321:\tlearn: 0.0090611\ttotal: 2m 54s\tremaining: 6m 6s\n",
      "322:\tlearn: 0.0090154\ttotal: 2m 54s\tremaining: 6m 6s\n",
      "323:\tlearn: 0.0089631\ttotal: 2m 55s\tremaining: 6m 5s\n",
      "324:\tlearn: 0.0089090\ttotal: 2m 56s\tremaining: 6m 5s\n",
      "325:\tlearn: 0.0088580\ttotal: 2m 56s\tremaining: 6m 5s\n",
      "326:\tlearn: 0.0088170\ttotal: 2m 57s\tremaining: 6m 4s\n",
      "327:\tlearn: 0.0087670\ttotal: 2m 57s\tremaining: 6m 4s\n",
      "328:\tlearn: 0.0085248\ttotal: 2m 58s\tremaining: 6m 4s\n",
      "329:\tlearn: 0.0084781\ttotal: 2m 59s\tremaining: 6m 4s\n",
      "330:\tlearn: 0.0084265\ttotal: 2m 59s\tremaining: 6m 3s\n",
      "331:\tlearn: 0.0083763\ttotal: 3m\tremaining: 6m 3s\n",
      "332:\tlearn: 0.0083344\ttotal: 3m 1s\tremaining: 6m 3s\n",
      "333:\tlearn: 0.0082887\ttotal: 3m 1s\tremaining: 6m 2s\n",
      "334:\tlearn: 0.0082319\ttotal: 3m 2s\tremaining: 6m 2s\n",
      "335:\tlearn: 0.0081873\ttotal: 3m 3s\tremaining: 6m 2s\n",
      "336:\tlearn: 0.0081397\ttotal: 3m 3s\tremaining: 6m 1s\n",
      "337:\tlearn: 0.0080825\ttotal: 3m 4s\tremaining: 6m 1s\n",
      "338:\tlearn: 0.0080358\ttotal: 3m 5s\tremaining: 6m 1s\n",
      "339:\tlearn: 0.0079955\ttotal: 3m 5s\tremaining: 6m\n",
      "340:\tlearn: 0.0079428\ttotal: 3m 6s\tremaining: 6m\n",
      "341:\tlearn: 0.0078965\ttotal: 3m 7s\tremaining: 6m\n",
      "342:\tlearn: 0.0078439\ttotal: 3m 7s\tremaining: 5m 59s\n",
      "343:\tlearn: 0.0077985\ttotal: 3m 8s\tremaining: 5m 59s\n",
      "344:\tlearn: 0.0077587\ttotal: 3m 9s\tremaining: 5m 59s\n",
      "345:\tlearn: 0.0077115\ttotal: 3m 9s\tremaining: 5m 59s\n",
      "346:\tlearn: 0.0076616\ttotal: 3m 10s\tremaining: 5m 58s\n",
      "347:\tlearn: 0.0076227\ttotal: 3m 11s\tremaining: 5m 58s\n",
      "348:\tlearn: 0.0075884\ttotal: 3m 12s\tremaining: 5m 58s\n",
      "349:\tlearn: 0.0075442\ttotal: 3m 12s\tremaining: 5m 57s\n",
      "350:\tlearn: 0.0075110\ttotal: 3m 13s\tremaining: 5m 57s\n",
      "351:\tlearn: 0.0074733\ttotal: 3m 14s\tremaining: 5m 57s\n",
      "352:\tlearn: 0.0074287\ttotal: 3m 14s\tremaining: 5m 57s\n",
      "353:\tlearn: 0.0073911\ttotal: 3m 15s\tremaining: 5m 56s\n",
      "354:\tlearn: 0.0073570\ttotal: 3m 16s\tremaining: 5m 56s\n",
      "355:\tlearn: 0.0073158\ttotal: 3m 16s\tremaining: 5m 55s\n",
      "356:\tlearn: 0.0072803\ttotal: 3m 17s\tremaining: 5m 55s\n",
      "357:\tlearn: 0.0072396\ttotal: 3m 18s\tremaining: 5m 55s\n",
      "358:\tlearn: 0.0072052\ttotal: 3m 18s\tremaining: 5m 54s\n",
      "359:\tlearn: 0.0071729\ttotal: 3m 19s\tremaining: 5m 54s\n",
      "360:\tlearn: 0.0071363\ttotal: 3m 19s\tremaining: 5m 53s\n",
      "361:\tlearn: 0.0071036\ttotal: 3m 20s\tremaining: 5m 53s\n",
      "362:\tlearn: 0.0070734\ttotal: 3m 21s\tremaining: 5m 53s\n",
      "363:\tlearn: 0.0070430\ttotal: 3m 21s\tremaining: 5m 52s\n",
      "364:\tlearn: 0.0068665\ttotal: 3m 22s\tremaining: 5m 52s\n",
      "365:\tlearn: 0.0068333\ttotal: 3m 23s\tremaining: 5m 52s\n",
      "366:\tlearn: 0.0067993\ttotal: 3m 23s\tremaining: 5m 51s\n",
      "367:\tlearn: 0.0067600\ttotal: 3m 24s\tremaining: 5m 51s\n",
      "368:\tlearn: 0.0067260\ttotal: 3m 25s\tremaining: 5m 50s\n",
      "369:\tlearn: 0.0066951\ttotal: 3m 25s\tremaining: 5m 50s\n",
      "370:\tlearn: 0.0066657\ttotal: 3m 26s\tremaining: 5m 49s\n",
      "371:\tlearn: 0.0066359\ttotal: 3m 27s\tremaining: 5m 49s\n",
      "372:\tlearn: 0.0066080\ttotal: 3m 27s\tremaining: 5m 49s\n",
      "373:\tlearn: 0.0065716\ttotal: 3m 28s\tremaining: 5m 48s\n",
      "374:\tlearn: 0.0065449\ttotal: 3m 28s\tremaining: 5m 48s\n",
      "375:\tlearn: 0.0065143\ttotal: 3m 29s\tremaining: 5m 47s\n",
      "376:\tlearn: 0.0064797\ttotal: 3m 30s\tremaining: 5m 47s\n",
      "377:\tlearn: 0.0064464\ttotal: 3m 30s\tremaining: 5m 47s\n",
      "378:\tlearn: 0.0064216\ttotal: 3m 31s\tremaining: 5m 46s\n",
      "379:\tlearn: 0.0063925\ttotal: 3m 32s\tremaining: 5m 46s\n",
      "380:\tlearn: 0.0063597\ttotal: 3m 32s\tremaining: 5m 45s\n",
      "381:\tlearn: 0.0063312\ttotal: 3m 33s\tremaining: 5m 45s\n",
      "382:\tlearn: 0.0063038\ttotal: 3m 34s\tremaining: 5m 45s\n",
      "383:\tlearn: 0.0062708\ttotal: 3m 35s\tremaining: 5m 44s\n",
      "384:\tlearn: 0.0062446\ttotal: 3m 35s\tremaining: 5m 44s\n",
      "385:\tlearn: 0.0062147\ttotal: 3m 36s\tremaining: 5m 44s\n",
      "386:\tlearn: 0.0061862\ttotal: 3m 36s\tremaining: 5m 43s\n",
      "387:\tlearn: 0.0061578\ttotal: 3m 37s\tremaining: 5m 43s\n",
      "388:\tlearn: 0.0061297\ttotal: 3m 38s\tremaining: 5m 42s\n",
      "389:\tlearn: 0.0061036\ttotal: 3m 38s\tremaining: 5m 42s\n",
      "390:\tlearn: 0.0060751\ttotal: 3m 39s\tremaining: 5m 41s\n",
      "391:\tlearn: 0.0060516\ttotal: 3m 40s\tremaining: 5m 41s\n",
      "392:\tlearn: 0.0060225\ttotal: 3m 40s\tremaining: 5m 40s\n",
      "393:\tlearn: 0.0059956\ttotal: 3m 41s\tremaining: 5m 40s\n",
      "394:\tlearn: 0.0059654\ttotal: 3m 42s\tremaining: 5m 40s\n",
      "395:\tlearn: 0.0059393\ttotal: 3m 42s\tremaining: 5m 39s\n",
      "396:\tlearn: 0.0059139\ttotal: 3m 43s\tremaining: 5m 39s\n",
      "397:\tlearn: 0.0058890\ttotal: 3m 44s\tremaining: 5m 38s\n",
      "398:\tlearn: 0.0058626\ttotal: 3m 44s\tremaining: 5m 38s\n",
      "399:\tlearn: 0.0058402\ttotal: 3m 45s\tremaining: 5m 38s\n",
      "400:\tlearn: 0.0058151\ttotal: 3m 46s\tremaining: 5m 37s\n",
      "401:\tlearn: 0.0057921\ttotal: 3m 46s\tremaining: 5m 37s\n",
      "402:\tlearn: 0.0057664\ttotal: 3m 47s\tremaining: 5m 36s\n",
      "403:\tlearn: 0.0057399\ttotal: 3m 48s\tremaining: 5m 36s\n",
      "404:\tlearn: 0.0057178\ttotal: 3m 48s\tremaining: 5m 36s\n",
      "405:\tlearn: 0.0056938\ttotal: 3m 49s\tremaining: 5m 35s\n",
      "406:\tlearn: 0.0056728\ttotal: 3m 50s\tremaining: 5m 35s\n",
      "407:\tlearn: 0.0056483\ttotal: 3m 50s\tremaining: 5m 34s\n",
      "408:\tlearn: 0.0056252\ttotal: 3m 51s\tremaining: 5m 34s\n",
      "409:\tlearn: 0.0056014\ttotal: 3m 52s\tremaining: 5m 34s\n",
      "410:\tlearn: 0.0055810\ttotal: 3m 52s\tremaining: 5m 33s\n",
      "411:\tlearn: 0.0055613\ttotal: 3m 53s\tremaining: 5m 33s\n",
      "412:\tlearn: 0.0055371\ttotal: 3m 54s\tremaining: 5m 32s\n",
      "413:\tlearn: 0.0055152\ttotal: 3m 54s\tremaining: 5m 32s\n",
      "414:\tlearn: 0.0054890\ttotal: 3m 55s\tremaining: 5m 31s\n",
      "415:\tlearn: 0.0054690\ttotal: 3m 55s\tremaining: 5m 31s\n",
      "416:\tlearn: 0.0054457\ttotal: 3m 56s\tremaining: 5m 30s\n",
      "417:\tlearn: 0.0054242\ttotal: 3m 57s\tremaining: 5m 30s\n",
      "418:\tlearn: 0.0054049\ttotal: 3m 57s\tremaining: 5m 29s\n",
      "419:\tlearn: 0.0053837\ttotal: 3m 58s\tremaining: 5m 29s\n",
      "420:\tlearn: 0.0053610\ttotal: 3m 59s\tremaining: 5m 28s\n",
      "421:\tlearn: 0.0053435\ttotal: 3m 59s\tremaining: 5m 28s\n",
      "422:\tlearn: 0.0053237\ttotal: 4m\tremaining: 5m 27s\n",
      "423:\tlearn: 0.0053004\ttotal: 4m 1s\tremaining: 5m 27s\n",
      "424:\tlearn: 0.0052815\ttotal: 4m 1s\tremaining: 5m 26s\n",
      "425:\tlearn: 0.0052610\ttotal: 4m 2s\tremaining: 5m 26s\n",
      "426:\tlearn: 0.0052401\ttotal: 4m 2s\tremaining: 5m 26s\n",
      "427:\tlearn: 0.0052202\ttotal: 4m 3s\tremaining: 5m 25s\n",
      "428:\tlearn: 0.0050954\ttotal: 4m 4s\tremaining: 5m 25s\n",
      "429:\tlearn: 0.0050785\ttotal: 4m 4s\tremaining: 5m 24s\n",
      "430:\tlearn: 0.0050581\ttotal: 4m 5s\tremaining: 5m 24s\n",
      "431:\tlearn: 0.0050367\ttotal: 4m 6s\tremaining: 5m 23s\n",
      "432:\tlearn: 0.0050160\ttotal: 4m 6s\tremaining: 5m 23s\n",
      "433:\tlearn: 0.0049965\ttotal: 4m 7s\tremaining: 5m 22s\n",
      "434:\tlearn: 0.0049769\ttotal: 4m 8s\tremaining: 5m 22s\n",
      "435:\tlearn: 0.0049600\ttotal: 4m 8s\tremaining: 5m 21s\n",
      "436:\tlearn: 0.0049379\ttotal: 4m 9s\tremaining: 5m 21s\n",
      "437:\tlearn: 0.0049173\ttotal: 4m 9s\tremaining: 5m 20s\n",
      "438:\tlearn: 0.0048987\ttotal: 4m 10s\tremaining: 5m 20s\n",
      "439:\tlearn: 0.0048771\ttotal: 4m 11s\tremaining: 5m 19s\n",
      "440:\tlearn: 0.0048591\ttotal: 4m 11s\tremaining: 5m 19s\n",
      "441:\tlearn: 0.0048388\ttotal: 4m 12s\tremaining: 5m 18s\n",
      "442:\tlearn: 0.0048211\ttotal: 4m 13s\tremaining: 5m 18s\n",
      "443:\tlearn: 0.0048026\ttotal: 4m 13s\tremaining: 5m 17s\n",
      "444:\tlearn: 0.0047854\ttotal: 4m 14s\tremaining: 5m 17s\n",
      "445:\tlearn: 0.0047663\ttotal: 4m 15s\tremaining: 5m 16s\n",
      "446:\tlearn: 0.0047472\ttotal: 4m 15s\tremaining: 5m 16s\n",
      "447:\tlearn: 0.0047313\ttotal: 4m 16s\tremaining: 5m 15s\n",
      "448:\tlearn: 0.0047129\ttotal: 4m 17s\tremaining: 5m 15s\n",
      "449:\tlearn: 0.0046941\ttotal: 4m 17s\tremaining: 5m 14s\n",
      "450:\tlearn: 0.0046756\ttotal: 4m 18s\tremaining: 5m 14s\n",
      "451:\tlearn: 0.0046576\ttotal: 4m 19s\tremaining: 5m 14s\n",
      "452:\tlearn: 0.0046403\ttotal: 4m 19s\tremaining: 5m 13s\n",
      "453:\tlearn: 0.0046247\ttotal: 4m 20s\tremaining: 5m 13s\n",
      "454:\tlearn: 0.0046099\ttotal: 4m 21s\tremaining: 5m 12s\n",
      "455:\tlearn: 0.0045930\ttotal: 4m 21s\tremaining: 5m 12s\n",
      "456:\tlearn: 0.0045796\ttotal: 4m 22s\tremaining: 5m 11s\n",
      "457:\tlearn: 0.0045604\ttotal: 4m 23s\tremaining: 5m 11s\n",
      "458:\tlearn: 0.0045447\ttotal: 4m 23s\tremaining: 5m 10s\n",
      "459:\tlearn: 0.0045263\ttotal: 4m 24s\tremaining: 5m 10s\n",
      "460:\tlearn: 0.0045104\ttotal: 4m 25s\tremaining: 5m 9s\n",
      "461:\tlearn: 0.0044955\ttotal: 4m 25s\tremaining: 5m 9s\n",
      "462:\tlearn: 0.0044795\ttotal: 4m 26s\tremaining: 5m 8s\n",
      "463:\tlearn: 0.0044660\ttotal: 4m 27s\tremaining: 5m 8s\n",
      "464:\tlearn: 0.0044479\ttotal: 4m 27s\tremaining: 5m 7s\n",
      "465:\tlearn: 0.0044322\ttotal: 4m 28s\tremaining: 5m 7s\n",
      "466:\tlearn: 0.0044170\ttotal: 4m 28s\tremaining: 5m 6s\n",
      "467:\tlearn: 0.0043201\ttotal: 4m 29s\tremaining: 5m 6s\n",
      "468:\tlearn: 0.0043070\ttotal: 4m 30s\tremaining: 5m 5s\n",
      "469:\tlearn: 0.0042944\ttotal: 4m 30s\tremaining: 5m 5s\n",
      "470:\tlearn: 0.0042776\ttotal: 4m 31s\tremaining: 5m 5s\n",
      "471:\tlearn: 0.0042636\ttotal: 4m 32s\tremaining: 5m 4s\n",
      "472:\tlearn: 0.0042481\ttotal: 4m 32s\tremaining: 5m 4s\n",
      "473:\tlearn: 0.0042341\ttotal: 4m 33s\tremaining: 5m 3s\n",
      "474:\tlearn: 0.0042176\ttotal: 4m 34s\tremaining: 5m 3s\n",
      "475:\tlearn: 0.0042033\ttotal: 4m 34s\tremaining: 5m 2s\n",
      "476:\tlearn: 0.0041872\ttotal: 4m 35s\tremaining: 5m 1s\n",
      "477:\tlearn: 0.0041732\ttotal: 4m 36s\tremaining: 5m 1s\n",
      "478:\tlearn: 0.0041582\ttotal: 4m 36s\tremaining: 5m\n",
      "479:\tlearn: 0.0041446\ttotal: 4m 37s\tremaining: 5m\n",
      "480:\tlearn: 0.0041306\ttotal: 4m 37s\tremaining: 4m 59s\n",
      "481:\tlearn: 0.0041166\ttotal: 4m 38s\tremaining: 4m 59s\n",
      "482:\tlearn: 0.0041017\ttotal: 4m 39s\tremaining: 4m 58s\n",
      "483:\tlearn: 0.0040880\ttotal: 4m 39s\tremaining: 4m 58s\n",
      "484:\tlearn: 0.0040731\ttotal: 4m 40s\tremaining: 4m 58s\n",
      "485:\tlearn: 0.0040615\ttotal: 4m 41s\tremaining: 4m 57s\n",
      "486:\tlearn: 0.0040480\ttotal: 4m 42s\tremaining: 4m 57s\n",
      "487:\tlearn: 0.0040355\ttotal: 4m 42s\tremaining: 4m 56s\n",
      "488:\tlearn: 0.0040224\ttotal: 4m 43s\tremaining: 4m 56s\n",
      "489:\tlearn: 0.0040088\ttotal: 4m 43s\tremaining: 4m 55s\n",
      "490:\tlearn: 0.0039960\ttotal: 4m 44s\tremaining: 4m 55s\n",
      "491:\tlearn: 0.0039814\ttotal: 4m 45s\tremaining: 4m 54s\n",
      "492:\tlearn: 0.0039682\ttotal: 4m 45s\tremaining: 4m 53s\n",
      "493:\tlearn: 0.0039573\ttotal: 4m 46s\tremaining: 4m 53s\n",
      "494:\tlearn: 0.0039439\ttotal: 4m 47s\tremaining: 4m 52s\n",
      "495:\tlearn: 0.0039318\ttotal: 4m 47s\tremaining: 4m 52s\n",
      "496:\tlearn: 0.0039205\ttotal: 4m 48s\tremaining: 4m 51s\n",
      "497:\tlearn: 0.0039080\ttotal: 4m 49s\tremaining: 4m 51s\n",
      "498:\tlearn: 0.0038961\ttotal: 4m 49s\tremaining: 4m 50s\n",
      "499:\tlearn: 0.0038842\ttotal: 4m 50s\tremaining: 4m 50s\n",
      "500:\tlearn: 0.0038722\ttotal: 4m 51s\tremaining: 4m 49s\n",
      "501:\tlearn: 0.0038613\ttotal: 4m 51s\tremaining: 4m 49s\n",
      "502:\tlearn: 0.0038498\ttotal: 4m 52s\tremaining: 4m 48s\n",
      "503:\tlearn: 0.0038399\ttotal: 4m 53s\tremaining: 4m 48s\n",
      "504:\tlearn: 0.0038276\ttotal: 4m 53s\tremaining: 4m 47s\n",
      "505:\tlearn: 0.0038137\ttotal: 4m 54s\tremaining: 4m 47s\n",
      "506:\tlearn: 0.0038017\ttotal: 4m 55s\tremaining: 4m 46s\n",
      "507:\tlearn: 0.0037900\ttotal: 4m 55s\tremaining: 4m 46s\n",
      "508:\tlearn: 0.0037785\ttotal: 4m 56s\tremaining: 4m 45s\n",
      "509:\tlearn: 0.0037654\ttotal: 4m 56s\tremaining: 4m 45s\n",
      "510:\tlearn: 0.0037553\ttotal: 4m 57s\tremaining: 4m 44s\n",
      "511:\tlearn: 0.0037445\ttotal: 4m 58s\tremaining: 4m 44s\n",
      "512:\tlearn: 0.0037328\ttotal: 4m 58s\tremaining: 4m 43s\n",
      "513:\tlearn: 0.0037203\ttotal: 4m 59s\tremaining: 4m 43s\n",
      "514:\tlearn: 0.0037103\ttotal: 5m\tremaining: 4m 42s\n",
      "515:\tlearn: 0.0036985\ttotal: 5m\tremaining: 4m 42s\n",
      "516:\tlearn: 0.0036876\ttotal: 5m 1s\tremaining: 4m 41s\n",
      "517:\tlearn: 0.0036772\ttotal: 5m 2s\tremaining: 4m 41s\n",
      "518:\tlearn: 0.0036685\ttotal: 5m 2s\tremaining: 4m 40s\n",
      "519:\tlearn: 0.0036554\ttotal: 5m 3s\tremaining: 4m 40s\n",
      "520:\tlearn: 0.0036454\ttotal: 5m 4s\tremaining: 4m 39s\n",
      "521:\tlearn: 0.0036348\ttotal: 5m 4s\tremaining: 4m 39s\n",
      "522:\tlearn: 0.0036224\ttotal: 5m 5s\tremaining: 4m 38s\n",
      "523:\tlearn: 0.0036149\ttotal: 5m 6s\tremaining: 4m 37s\n",
      "524:\tlearn: 0.0036057\ttotal: 5m 6s\tremaining: 4m 37s\n",
      "525:\tlearn: 0.0035966\ttotal: 5m 7s\tremaining: 4m 36s\n",
      "526:\tlearn: 0.0035852\ttotal: 5m 7s\tremaining: 4m 36s\n",
      "527:\tlearn: 0.0035740\ttotal: 5m 8s\tremaining: 4m 35s\n",
      "528:\tlearn: 0.0035646\ttotal: 5m 9s\tremaining: 4m 35s\n",
      "529:\tlearn: 0.0035559\ttotal: 5m 9s\tremaining: 4m 34s\n",
      "530:\tlearn: 0.0035466\ttotal: 5m 10s\tremaining: 4m 34s\n",
      "531:\tlearn: 0.0035367\ttotal: 5m 11s\tremaining: 4m 33s\n",
      "532:\tlearn: 0.0035258\ttotal: 5m 11s\tremaining: 4m 33s\n",
      "533:\tlearn: 0.0035167\ttotal: 5m 12s\tremaining: 4m 32s\n",
      "534:\tlearn: 0.0035049\ttotal: 5m 13s\tremaining: 4m 32s\n",
      "535:\tlearn: 0.0034966\ttotal: 5m 13s\tremaining: 4m 31s\n",
      "536:\tlearn: 0.0034864\ttotal: 5m 14s\tremaining: 4m 31s\n",
      "537:\tlearn: 0.0034771\ttotal: 5m 15s\tremaining: 4m 30s\n",
      "538:\tlearn: 0.0034674\ttotal: 5m 15s\tremaining: 4m 30s\n",
      "539:\tlearn: 0.0034594\ttotal: 5m 16s\tremaining: 4m 29s\n",
      "540:\tlearn: 0.0034501\ttotal: 5m 17s\tremaining: 4m 28s\n",
      "541:\tlearn: 0.0034392\ttotal: 5m 17s\tremaining: 4m 28s\n",
      "542:\tlearn: 0.0034300\ttotal: 5m 18s\tremaining: 4m 27s\n",
      "543:\tlearn: 0.0034201\ttotal: 5m 19s\tremaining: 4m 27s\n",
      "544:\tlearn: 0.0034108\ttotal: 5m 19s\tremaining: 4m 26s\n",
      "545:\tlearn: 0.0034014\ttotal: 5m 20s\tremaining: 4m 26s\n",
      "546:\tlearn: 0.0033934\ttotal: 5m 21s\tremaining: 4m 25s\n",
      "547:\tlearn: 0.0033851\ttotal: 5m 21s\tremaining: 4m 25s\n",
      "548:\tlearn: 0.0033751\ttotal: 5m 22s\tremaining: 4m 24s\n",
      "549:\tlearn: 0.0033674\ttotal: 5m 23s\tremaining: 4m 24s\n",
      "550:\tlearn: 0.0033586\ttotal: 5m 23s\tremaining: 4m 23s\n",
      "551:\tlearn: 0.0033499\ttotal: 5m 24s\tremaining: 4m 23s\n",
      "552:\tlearn: 0.0033405\ttotal: 5m 25s\tremaining: 4m 22s\n",
      "553:\tlearn: 0.0033323\ttotal: 5m 25s\tremaining: 4m 22s\n",
      "554:\tlearn: 0.0033231\ttotal: 5m 26s\tremaining: 4m 21s\n",
      "555:\tlearn: 0.0033154\ttotal: 5m 26s\tremaining: 4m 21s\n",
      "556:\tlearn: 0.0032435\ttotal: 5m 27s\tremaining: 4m 20s\n",
      "557:\tlearn: 0.0032342\ttotal: 5m 28s\tremaining: 4m 20s\n",
      "558:\tlearn: 0.0032260\ttotal: 5m 28s\tremaining: 4m 19s\n",
      "559:\tlearn: 0.0032163\ttotal: 5m 29s\tremaining: 4m 18s\n",
      "560:\tlearn: 0.0032079\ttotal: 5m 30s\tremaining: 4m 18s\n",
      "561:\tlearn: 0.0032003\ttotal: 5m 30s\tremaining: 4m 17s\n",
      "562:\tlearn: 0.0031931\ttotal: 5m 31s\tremaining: 4m 17s\n",
      "563:\tlearn: 0.0031847\ttotal: 5m 32s\tremaining: 4m 16s\n",
      "564:\tlearn: 0.0031751\ttotal: 5m 32s\tremaining: 4m 16s\n",
      "565:\tlearn: 0.0031661\ttotal: 5m 33s\tremaining: 4m 15s\n",
      "566:\tlearn: 0.0031583\ttotal: 5m 34s\tremaining: 4m 15s\n",
      "567:\tlearn: 0.0031509\ttotal: 5m 34s\tremaining: 4m 14s\n",
      "568:\tlearn: 0.0031408\ttotal: 5m 35s\tremaining: 4m 14s\n",
      "569:\tlearn: 0.0031318\ttotal: 5m 36s\tremaining: 4m 13s\n",
      "570:\tlearn: 0.0031244\ttotal: 5m 37s\tremaining: 4m 13s\n",
      "571:\tlearn: 0.0031172\ttotal: 5m 37s\tremaining: 4m 12s\n",
      "572:\tlearn: 0.0031090\ttotal: 5m 38s\tremaining: 4m 12s\n",
      "573:\tlearn: 0.0031007\ttotal: 5m 39s\tremaining: 4m 11s\n",
      "574:\tlearn: 0.0030926\ttotal: 5m 39s\tremaining: 4m 11s\n",
      "575:\tlearn: 0.0030857\ttotal: 5m 40s\tremaining: 4m 10s\n",
      "576:\tlearn: 0.0030785\ttotal: 5m 41s\tremaining: 4m 10s\n",
      "577:\tlearn: 0.0030710\ttotal: 5m 41s\tremaining: 4m 9s\n",
      "578:\tlearn: 0.0030619\ttotal: 5m 42s\tremaining: 4m 9s\n",
      "579:\tlearn: 0.0030552\ttotal: 5m 43s\tremaining: 4m 8s\n",
      "580:\tlearn: 0.0030488\ttotal: 5m 44s\tremaining: 4m 8s\n",
      "581:\tlearn: 0.0030411\ttotal: 5m 44s\tremaining: 4m 7s\n",
      "582:\tlearn: 0.0030332\ttotal: 5m 45s\tremaining: 4m 7s\n",
      "583:\tlearn: 0.0030265\ttotal: 5m 46s\tremaining: 4m 6s\n",
      "584:\tlearn: 0.0030186\ttotal: 5m 46s\tremaining: 4m 6s\n",
      "585:\tlearn: 0.0030105\ttotal: 5m 47s\tremaining: 4m 5s\n",
      "586:\tlearn: 0.0030038\ttotal: 5m 48s\tremaining: 4m 5s\n",
      "587:\tlearn: 0.0029954\ttotal: 5m 48s\tremaining: 4m 4s\n",
      "588:\tlearn: 0.0029891\ttotal: 5m 49s\tremaining: 4m 4s\n",
      "589:\tlearn: 0.0029832\ttotal: 5m 50s\tremaining: 4m 3s\n",
      "590:\tlearn: 0.0029762\ttotal: 5m 51s\tremaining: 4m 3s\n",
      "591:\tlearn: 0.0029690\ttotal: 5m 51s\tremaining: 4m 2s\n",
      "592:\tlearn: 0.0029611\ttotal: 5m 52s\tremaining: 4m 1s\n",
      "593:\tlearn: 0.0029548\ttotal: 5m 53s\tremaining: 4m 1s\n",
      "594:\tlearn: 0.0029467\ttotal: 5m 53s\tremaining: 4m\n",
      "595:\tlearn: 0.0029403\ttotal: 5m 54s\tremaining: 4m\n",
      "596:\tlearn: 0.0029337\ttotal: 5m 55s\tremaining: 3m 59s\n",
      "597:\tlearn: 0.0029256\ttotal: 5m 55s\tremaining: 3m 59s\n",
      "598:\tlearn: 0.0029193\ttotal: 5m 56s\tremaining: 3m 58s\n",
      "599:\tlearn: 0.0029122\ttotal: 5m 56s\tremaining: 3m 57s\n",
      "600:\tlearn: 0.0029063\ttotal: 5m 57s\tremaining: 3m 57s\n",
      "601:\tlearn: 0.0028993\ttotal: 5m 58s\tremaining: 3m 56s\n",
      "602:\tlearn: 0.0028914\ttotal: 5m 58s\tremaining: 3m 56s\n",
      "603:\tlearn: 0.0028844\ttotal: 5m 59s\tremaining: 3m 55s\n",
      "604:\tlearn: 0.0028774\ttotal: 6m\tremaining: 3m 55s\n",
      "605:\tlearn: 0.0028711\ttotal: 6m\tremaining: 3m 54s\n",
      "606:\tlearn: 0.0028663\ttotal: 6m 1s\tremaining: 3m 53s\n",
      "607:\tlearn: 0.0028601\ttotal: 6m 1s\tremaining: 3m 53s\n",
      "608:\tlearn: 0.0028535\ttotal: 6m 2s\tremaining: 3m 52s\n",
      "609:\tlearn: 0.0028479\ttotal: 6m 3s\tremaining: 3m 52s\n",
      "610:\tlearn: 0.0028420\ttotal: 6m 3s\tremaining: 3m 51s\n",
      "611:\tlearn: 0.0028375\ttotal: 6m 4s\tremaining: 3m 51s\n",
      "612:\tlearn: 0.0028331\ttotal: 6m 5s\tremaining: 3m 50s\n",
      "613:\tlearn: 0.0028288\ttotal: 6m 5s\tremaining: 3m 49s\n",
      "614:\tlearn: 0.0028208\ttotal: 6m 6s\tremaining: 3m 49s\n",
      "615:\tlearn: 0.0028154\ttotal: 6m 7s\tremaining: 3m 48s\n",
      "616:\tlearn: 0.0028087\ttotal: 6m 7s\tremaining: 3m 48s\n",
      "617:\tlearn: 0.0028024\ttotal: 6m 8s\tremaining: 3m 47s\n",
      "618:\tlearn: 0.0027948\ttotal: 6m 8s\tremaining: 3m 47s\n",
      "619:\tlearn: 0.0027869\ttotal: 6m 9s\tremaining: 3m 46s\n",
      "620:\tlearn: 0.0027808\ttotal: 6m 10s\tremaining: 3m 45s\n",
      "621:\tlearn: 0.0027743\ttotal: 6m 10s\tremaining: 3m 45s\n",
      "622:\tlearn: 0.0027681\ttotal: 6m 11s\tremaining: 3m 44s\n",
      "623:\tlearn: 0.0027611\ttotal: 6m 12s\tremaining: 3m 44s\n",
      "624:\tlearn: 0.0027543\ttotal: 6m 12s\tremaining: 3m 43s\n",
      "625:\tlearn: 0.0027475\ttotal: 6m 13s\tremaining: 3m 43s\n",
      "626:\tlearn: 0.0027401\ttotal: 6m 13s\tremaining: 3m 42s\n",
      "627:\tlearn: 0.0027335\ttotal: 6m 14s\tremaining: 3m 41s\n",
      "628:\tlearn: 0.0027277\ttotal: 6m 15s\tremaining: 3m 41s\n",
      "629:\tlearn: 0.0027226\ttotal: 6m 15s\tremaining: 3m 40s\n",
      "630:\tlearn: 0.0027165\ttotal: 6m 16s\tremaining: 3m 40s\n",
      "631:\tlearn: 0.0027117\ttotal: 6m 17s\tremaining: 3m 39s\n",
      "632:\tlearn: 0.0027054\ttotal: 6m 17s\tremaining: 3m 39s\n",
      "633:\tlearn: 0.0026995\ttotal: 6m 18s\tremaining: 3m 38s\n",
      "634:\tlearn: 0.0026937\ttotal: 6m 19s\tremaining: 3m 37s\n",
      "635:\tlearn: 0.0026875\ttotal: 6m 19s\tremaining: 3m 37s\n",
      "636:\tlearn: 0.0026826\ttotal: 6m 20s\tremaining: 3m 36s\n",
      "637:\tlearn: 0.0026767\ttotal: 6m 21s\tremaining: 3m 36s\n",
      "638:\tlearn: 0.0026717\ttotal: 6m 21s\tremaining: 3m 35s\n",
      "639:\tlearn: 0.0026658\ttotal: 6m 22s\tremaining: 3m 35s\n",
      "640:\tlearn: 0.0026601\ttotal: 6m 23s\tremaining: 3m 34s\n",
      "641:\tlearn: 0.0026544\ttotal: 6m 23s\tremaining: 3m 34s\n",
      "642:\tlearn: 0.0026476\ttotal: 6m 24s\tremaining: 3m 33s\n",
      "643:\tlearn: 0.0026413\ttotal: 6m 25s\tremaining: 3m 32s\n",
      "644:\tlearn: 0.0026349\ttotal: 6m 25s\tremaining: 3m 32s\n",
      "645:\tlearn: 0.0026282\ttotal: 6m 26s\tremaining: 3m 31s\n",
      "646:\tlearn: 0.0026221\ttotal: 6m 27s\tremaining: 3m 31s\n",
      "647:\tlearn: 0.0026221\ttotal: 6m 27s\tremaining: 3m 30s\n",
      "648:\tlearn: 0.0026177\ttotal: 6m 27s\tremaining: 3m 29s\n",
      "649:\tlearn: 0.0026123\ttotal: 6m 28s\tremaining: 3m 29s\n",
      "650:\tlearn: 0.0026064\ttotal: 6m 29s\tremaining: 3m 28s\n",
      "651:\tlearn: 0.0026022\ttotal: 6m 29s\tremaining: 3m 28s\n",
      "652:\tlearn: 0.0025970\ttotal: 6m 30s\tremaining: 3m 27s\n",
      "653:\tlearn: 0.0025919\ttotal: 6m 31s\tremaining: 3m 26s\n",
      "654:\tlearn: 0.0025872\ttotal: 6m 31s\tremaining: 3m 26s\n",
      "655:\tlearn: 0.0025817\ttotal: 6m 32s\tremaining: 3m 25s\n",
      "656:\tlearn: 0.0025817\ttotal: 6m 32s\tremaining: 3m 24s\n",
      "657:\tlearn: 0.0025759\ttotal: 6m 33s\tremaining: 3m 24s\n",
      "658:\tlearn: 0.0025702\ttotal: 6m 33s\tremaining: 3m 23s\n",
      "659:\tlearn: 0.0025644\ttotal: 6m 34s\tremaining: 3m 23s\n",
      "660:\tlearn: 0.0025587\ttotal: 6m 35s\tremaining: 3m 22s\n",
      "661:\tlearn: 0.0025587\ttotal: 6m 35s\tremaining: 3m 21s\n",
      "662:\tlearn: 0.0025587\ttotal: 6m 35s\tremaining: 3m 21s\n",
      "663:\tlearn: 0.0025542\ttotal: 6m 36s\tremaining: 3m 20s\n",
      "664:\tlearn: 0.0025487\ttotal: 6m 36s\tremaining: 3m 19s\n",
      "665:\tlearn: 0.0025487\ttotal: 6m 37s\tremaining: 3m 19s\n",
      "666:\tlearn: 0.0025434\ttotal: 6m 37s\tremaining: 3m 18s\n",
      "667:\tlearn: 0.0025384\ttotal: 6m 38s\tremaining: 3m 18s\n",
      "668:\tlearn: 0.0025324\ttotal: 6m 39s\tremaining: 3m 17s\n",
      "669:\tlearn: 0.0025284\ttotal: 6m 39s\tremaining: 3m 16s\n",
      "670:\tlearn: 0.0025227\ttotal: 6m 40s\tremaining: 3m 16s\n",
      "671:\tlearn: 0.0025167\ttotal: 6m 41s\tremaining: 3m 15s\n",
      "672:\tlearn: 0.0025167\ttotal: 6m 41s\tremaining: 3m 14s\n",
      "673:\tlearn: 0.0025110\ttotal: 6m 41s\tremaining: 3m 14s\n",
      "674:\tlearn: 0.0025059\ttotal: 6m 42s\tremaining: 3m 13s\n",
      "675:\tlearn: 0.0025004\ttotal: 6m 43s\tremaining: 3m 13s\n",
      "676:\tlearn: 0.0024949\ttotal: 6m 43s\tremaining: 3m 12s\n",
      "677:\tlearn: 0.0024893\ttotal: 6m 44s\tremaining: 3m 12s\n",
      "678:\tlearn: 0.0024840\ttotal: 6m 44s\tremaining: 3m 11s\n",
      "679:\tlearn: 0.0024791\ttotal: 6m 45s\tremaining: 3m 10s\n",
      "680:\tlearn: 0.0024745\ttotal: 6m 46s\tremaining: 3m 10s\n",
      "681:\tlearn: 0.0024692\ttotal: 6m 46s\tremaining: 3m 9s\n",
      "682:\tlearn: 0.0024638\ttotal: 6m 47s\tremaining: 3m 9s\n",
      "683:\tlearn: 0.0024638\ttotal: 6m 47s\tremaining: 3m 8s\n",
      "684:\tlearn: 0.0024588\ttotal: 6m 48s\tremaining: 3m 7s\n",
      "685:\tlearn: 0.0024588\ttotal: 6m 48s\tremaining: 3m 7s\n",
      "686:\tlearn: 0.0024543\ttotal: 6m 49s\tremaining: 3m 6s\n",
      "687:\tlearn: 0.0024490\ttotal: 6m 49s\tremaining: 3m 5s\n",
      "688:\tlearn: 0.0024477\ttotal: 6m 50s\tremaining: 3m 5s\n",
      "689:\tlearn: 0.0024437\ttotal: 6m 50s\tremaining: 3m 4s\n",
      "690:\tlearn: 0.0024390\ttotal: 6m 51s\tremaining: 3m 3s\n",
      "691:\tlearn: 0.0024344\ttotal: 6m 51s\tremaining: 3m 3s\n",
      "692:\tlearn: 0.0024293\ttotal: 6m 52s\tremaining: 3m 2s\n",
      "693:\tlearn: 0.0024293\ttotal: 6m 52s\tremaining: 3m 2s\n",
      "694:\tlearn: 0.0024293\ttotal: 6m 53s\tremaining: 3m 1s\n",
      "695:\tlearn: 0.0024243\ttotal: 6m 53s\tremaining: 3m\n",
      "696:\tlearn: 0.0024200\ttotal: 6m 54s\tremaining: 3m\n",
      "697:\tlearn: 0.0024148\ttotal: 6m 54s\tremaining: 2m 59s\n",
      "698:\tlearn: 0.0024101\ttotal: 6m 55s\tremaining: 2m 58s\n",
      "699:\tlearn: 0.0023917\ttotal: 6m 56s\tremaining: 2m 58s\n",
      "700:\tlearn: 0.0023917\ttotal: 6m 56s\tremaining: 2m 57s\n",
      "701:\tlearn: 0.0023872\ttotal: 6m 57s\tremaining: 2m 57s\n",
      "702:\tlearn: 0.0023826\ttotal: 6m 57s\tremaining: 2m 56s\n",
      "703:\tlearn: 0.0023826\ttotal: 6m 57s\tremaining: 2m 55s\n",
      "704:\tlearn: 0.0023826\ttotal: 6m 58s\tremaining: 2m 54s\n",
      "705:\tlearn: 0.0023826\ttotal: 6m 58s\tremaining: 2m 54s\n",
      "706:\tlearn: 0.0023825\ttotal: 6m 58s\tremaining: 2m 53s\n",
      "707:\tlearn: 0.0023825\ttotal: 6m 58s\tremaining: 2m 52s\n",
      "708:\tlearn: 0.0023825\ttotal: 6m 59s\tremaining: 2m 52s\n",
      "709:\tlearn: 0.0023824\ttotal: 6m 59s\tremaining: 2m 51s\n",
      "710:\tlearn: 0.0023824\ttotal: 6m 59s\tremaining: 2m 50s\n",
      "711:\tlearn: 0.0023780\ttotal: 7m\tremaining: 2m 50s\n",
      "712:\tlearn: 0.0023736\ttotal: 7m 1s\tremaining: 2m 49s\n",
      "713:\tlearn: 0.0023736\ttotal: 7m 1s\tremaining: 2m 48s\n",
      "714:\tlearn: 0.0023736\ttotal: 7m 1s\tremaining: 2m 48s\n",
      "715:\tlearn: 0.0023688\ttotal: 7m 2s\tremaining: 2m 47s\n",
      "716:\tlearn: 0.0023688\ttotal: 7m 2s\tremaining: 2m 46s\n",
      "717:\tlearn: 0.0023663\ttotal: 7m 2s\tremaining: 2m 46s\n",
      "718:\tlearn: 0.0023663\ttotal: 7m 2s\tremaining: 2m 45s\n",
      "719:\tlearn: 0.0023616\ttotal: 7m 3s\tremaining: 2m 44s\n",
      "720:\tlearn: 0.0023616\ttotal: 7m 3s\tremaining: 2m 44s\n",
      "721:\tlearn: 0.0023616\ttotal: 7m 4s\tremaining: 2m 43s\n",
      "722:\tlearn: 0.0023616\ttotal: 7m 4s\tremaining: 2m 42s\n",
      "723:\tlearn: 0.0023616\ttotal: 7m 4s\tremaining: 2m 41s\n",
      "724:\tlearn: 0.0023616\ttotal: 7m 4s\tremaining: 2m 41s\n",
      "725:\tlearn: 0.0023616\ttotal: 7m 4s\tremaining: 2m 40s\n",
      "726:\tlearn: 0.0023616\ttotal: 7m 5s\tremaining: 2m 39s\n",
      "727:\tlearn: 0.0023616\ttotal: 7m 5s\tremaining: 2m 38s\n",
      "728:\tlearn: 0.0023616\ttotal: 7m 5s\tremaining: 2m 38s\n",
      "729:\tlearn: 0.0023616\ttotal: 7m 5s\tremaining: 2m 37s\n",
      "730:\tlearn: 0.0023616\ttotal: 7m 6s\tremaining: 2m 36s\n",
      "731:\tlearn: 0.0023616\ttotal: 7m 6s\tremaining: 2m 36s\n",
      "732:\tlearn: 0.0023616\ttotal: 7m 6s\tremaining: 2m 35s\n",
      "733:\tlearn: 0.0023616\ttotal: 7m 6s\tremaining: 2m 34s\n",
      "734:\tlearn: 0.0023616\ttotal: 7m 6s\tremaining: 2m 33s\n",
      "735:\tlearn: 0.0023570\ttotal: 7m 7s\tremaining: 2m 33s\n",
      "736:\tlearn: 0.0023570\ttotal: 7m 7s\tremaining: 2m 32s\n",
      "737:\tlearn: 0.0023569\ttotal: 7m 7s\tremaining: 2m 31s\n",
      "738:\tlearn: 0.0023569\ttotal: 7m 8s\tremaining: 2m 31s\n",
      "739:\tlearn: 0.0023569\ttotal: 7m 8s\tremaining: 2m 30s\n",
      "740:\tlearn: 0.0023525\ttotal: 7m 9s\tremaining: 2m 30s\n",
      "741:\tlearn: 0.0023525\ttotal: 7m 9s\tremaining: 2m 29s\n",
      "742:\tlearn: 0.0023476\ttotal: 7m 10s\tremaining: 2m 28s\n",
      "743:\tlearn: 0.0023476\ttotal: 7m 10s\tremaining: 2m 28s\n",
      "744:\tlearn: 0.0023066\ttotal: 7m 10s\tremaining: 2m 27s\n",
      "745:\tlearn: 0.0023017\ttotal: 7m 11s\tremaining: 2m 26s\n",
      "746:\tlearn: 0.0022974\ttotal: 7m 12s\tremaining: 2m 26s\n",
      "747:\tlearn: 0.0022932\ttotal: 7m 12s\tremaining: 2m 25s\n",
      "748:\tlearn: 0.0022932\ttotal: 7m 13s\tremaining: 2m 25s\n",
      "749:\tlearn: 0.0022883\ttotal: 7m 13s\tremaining: 2m 24s\n",
      "750:\tlearn: 0.0022839\ttotal: 7m 14s\tremaining: 2m 24s\n",
      "751:\tlearn: 0.0022839\ttotal: 7m 14s\tremaining: 2m 23s\n",
      "752:\tlearn: 0.0022795\ttotal: 7m 15s\tremaining: 2m 22s\n",
      "753:\tlearn: 0.0022747\ttotal: 7m 16s\tremaining: 2m 22s\n",
      "754:\tlearn: 0.0022700\ttotal: 7m 16s\tremaining: 2m 21s\n",
      "755:\tlearn: 0.0022653\ttotal: 7m 17s\tremaining: 2m 21s\n",
      "756:\tlearn: 0.0022653\ttotal: 7m 17s\tremaining: 2m 20s\n",
      "757:\tlearn: 0.0022614\ttotal: 7m 18s\tremaining: 2m 19s\n",
      "758:\tlearn: 0.0022571\ttotal: 7m 18s\tremaining: 2m 19s\n",
      "759:\tlearn: 0.0022527\ttotal: 7m 19s\tremaining: 2m 18s\n",
      "760:\tlearn: 0.0022484\ttotal: 7m 19s\tremaining: 2m 18s\n",
      "761:\tlearn: 0.0022436\ttotal: 7m 20s\tremaining: 2m 17s\n",
      "762:\tlearn: 0.0022396\ttotal: 7m 21s\tremaining: 2m 17s\n",
      "763:\tlearn: 0.0022396\ttotal: 7m 21s\tremaining: 2m 16s\n",
      "764:\tlearn: 0.0022378\ttotal: 7m 21s\tremaining: 2m 15s\n",
      "765:\tlearn: 0.0022333\ttotal: 7m 22s\tremaining: 2m 15s\n",
      "766:\tlearn: 0.0022293\ttotal: 7m 23s\tremaining: 2m 14s\n",
      "767:\tlearn: 0.0022293\ttotal: 7m 23s\tremaining: 2m 13s\n",
      "768:\tlearn: 0.0022251\ttotal: 7m 24s\tremaining: 2m 13s\n",
      "769:\tlearn: 0.0022214\ttotal: 7m 24s\tremaining: 2m 12s\n",
      "770:\tlearn: 0.0022214\ttotal: 7m 25s\tremaining: 2m 12s\n",
      "771:\tlearn: 0.0022176\ttotal: 7m 25s\tremaining: 2m 11s\n",
      "772:\tlearn: 0.0022133\ttotal: 7m 26s\tremaining: 2m 11s\n",
      "773:\tlearn: 0.0022097\ttotal: 7m 26s\tremaining: 2m 10s\n",
      "774:\tlearn: 0.0022062\ttotal: 7m 27s\tremaining: 2m 9s\n",
      "775:\tlearn: 0.0022023\ttotal: 7m 28s\tremaining: 2m 9s\n",
      "776:\tlearn: 0.0021984\ttotal: 7m 28s\tremaining: 2m 8s\n",
      "777:\tlearn: 0.0021946\ttotal: 7m 29s\tremaining: 2m 8s\n",
      "778:\tlearn: 0.0021946\ttotal: 7m 29s\tremaining: 2m 7s\n",
      "779:\tlearn: 0.0021946\ttotal: 7m 29s\tremaining: 2m 6s\n",
      "780:\tlearn: 0.0021909\ttotal: 7m 30s\tremaining: 2m 6s\n",
      "781:\tlearn: 0.0021909\ttotal: 7m 30s\tremaining: 2m 5s\n",
      "782:\tlearn: 0.0021909\ttotal: 7m 30s\tremaining: 2m 4s\n",
      "783:\tlearn: 0.0021909\ttotal: 7m 31s\tremaining: 2m 4s\n",
      "784:\tlearn: 0.0021909\ttotal: 7m 31s\tremaining: 2m 3s\n",
      "785:\tlearn: 0.0021867\ttotal: 7m 32s\tremaining: 2m 3s\n",
      "786:\tlearn: 0.0021867\ttotal: 7m 32s\tremaining: 2m 2s\n",
      "787:\tlearn: 0.0021826\ttotal: 7m 32s\tremaining: 2m 1s\n",
      "788:\tlearn: 0.0021826\ttotal: 7m 33s\tremaining: 2m 1s\n",
      "789:\tlearn: 0.0021783\ttotal: 7m 33s\tremaining: 2m\n",
      "790:\tlearn: 0.0021740\ttotal: 7m 34s\tremaining: 2m\n",
      "791:\tlearn: 0.0021701\ttotal: 7m 35s\tremaining: 1m 59s\n",
      "792:\tlearn: 0.0021667\ttotal: 7m 35s\tremaining: 1m 58s\n",
      "793:\tlearn: 0.0021629\ttotal: 7m 36s\tremaining: 1m 58s\n",
      "794:\tlearn: 0.0021594\ttotal: 7m 37s\tremaining: 1m 57s\n",
      "795:\tlearn: 0.0021594\ttotal: 7m 37s\tremaining: 1m 57s\n",
      "796:\tlearn: 0.0021594\ttotal: 7m 37s\tremaining: 1m 56s\n",
      "797:\tlearn: 0.0021594\ttotal: 7m 37s\tremaining: 1m 55s\n",
      "798:\tlearn: 0.0021559\ttotal: 7m 38s\tremaining: 1m 55s\n",
      "799:\tlearn: 0.0021559\ttotal: 7m 38s\tremaining: 1m 54s\n",
      "800:\tlearn: 0.0021559\ttotal: 7m 38s\tremaining: 1m 54s\n",
      "801:\tlearn: 0.0021522\ttotal: 7m 39s\tremaining: 1m 53s\n",
      "802:\tlearn: 0.0021522\ttotal: 7m 39s\tremaining: 1m 52s\n",
      "803:\tlearn: 0.0021522\ttotal: 7m 40s\tremaining: 1m 52s\n",
      "804:\tlearn: 0.0021523\ttotal: 7m 40s\tremaining: 1m 51s\n",
      "805:\tlearn: 0.0021523\ttotal: 7m 40s\tremaining: 1m 50s\n",
      "806:\tlearn: 0.0021486\ttotal: 7m 41s\tremaining: 1m 50s\n",
      "807:\tlearn: 0.0021486\ttotal: 7m 41s\tremaining: 1m 49s\n",
      "808:\tlearn: 0.0021486\ttotal: 7m 41s\tremaining: 1m 49s\n",
      "809:\tlearn: 0.0021449\ttotal: 7m 42s\tremaining: 1m 48s\n",
      "810:\tlearn: 0.0021449\ttotal: 7m 42s\tremaining: 1m 47s\n",
      "811:\tlearn: 0.0021449\ttotal: 7m 42s\tremaining: 1m 47s\n",
      "812:\tlearn: 0.0021449\ttotal: 7m 43s\tremaining: 1m 46s\n",
      "813:\tlearn: 0.0021449\ttotal: 7m 43s\tremaining: 1m 45s\n",
      "814:\tlearn: 0.0021449\ttotal: 7m 43s\tremaining: 1m 45s\n",
      "815:\tlearn: 0.0021408\ttotal: 7m 44s\tremaining: 1m 44s\n",
      "816:\tlearn: 0.0021369\ttotal: 7m 44s\tremaining: 1m 44s\n",
      "817:\tlearn: 0.0021369\ttotal: 7m 45s\tremaining: 1m 43s\n",
      "818:\tlearn: 0.0021333\ttotal: 7m 45s\tremaining: 1m 42s\n",
      "819:\tlearn: 0.0021319\ttotal: 7m 46s\tremaining: 1m 42s\n",
      "820:\tlearn: 0.0021319\ttotal: 7m 46s\tremaining: 1m 41s\n",
      "821:\tlearn: 0.0021319\ttotal: 7m 46s\tremaining: 1m 41s\n",
      "822:\tlearn: 0.0021319\ttotal: 7m 47s\tremaining: 1m 40s\n",
      "823:\tlearn: 0.0021319\ttotal: 7m 47s\tremaining: 1m 39s\n",
      "824:\tlearn: 0.0021319\ttotal: 7m 47s\tremaining: 1m 39s\n",
      "825:\tlearn: 0.0021288\ttotal: 7m 48s\tremaining: 1m 38s\n",
      "826:\tlearn: 0.0021288\ttotal: 7m 48s\tremaining: 1m 37s\n",
      "827:\tlearn: 0.0021288\ttotal: 7m 48s\tremaining: 1m 37s\n",
      "828:\tlearn: 0.0021248\ttotal: 7m 49s\tremaining: 1m 36s\n",
      "829:\tlearn: 0.0021207\ttotal: 7m 49s\tremaining: 1m 36s\n",
      "830:\tlearn: 0.0021207\ttotal: 7m 50s\tremaining: 1m 35s\n",
      "831:\tlearn: 0.0021207\ttotal: 7m 50s\tremaining: 1m 34s\n",
      "832:\tlearn: 0.0021167\ttotal: 7m 51s\tremaining: 1m 34s\n",
      "833:\tlearn: 0.0021129\ttotal: 7m 51s\tremaining: 1m 33s\n",
      "834:\tlearn: 0.0021090\ttotal: 7m 52s\tremaining: 1m 33s\n",
      "835:\tlearn: 0.0021090\ttotal: 7m 52s\tremaining: 1m 32s\n",
      "836:\tlearn: 0.0021090\ttotal: 7m 52s\tremaining: 1m 32s\n",
      "837:\tlearn: 0.0021047\ttotal: 7m 53s\tremaining: 1m 31s\n",
      "838:\tlearn: 0.0021004\ttotal: 7m 54s\tremaining: 1m 30s\n",
      "839:\tlearn: 0.0021004\ttotal: 7m 54s\tremaining: 1m 30s\n",
      "840:\tlearn: 0.0021004\ttotal: 7m 54s\tremaining: 1m 29s\n",
      "841:\tlearn: 0.0021004\ttotal: 7m 54s\tremaining: 1m 29s\n",
      "842:\tlearn: 0.0020967\ttotal: 7m 55s\tremaining: 1m 28s\n",
      "843:\tlearn: 0.0020967\ttotal: 7m 55s\tremaining: 1m 27s\n",
      "844:\tlearn: 0.0020925\ttotal: 7m 56s\tremaining: 1m 27s\n",
      "845:\tlearn: 0.0020925\ttotal: 7m 56s\tremaining: 1m 26s\n",
      "846:\tlearn: 0.0020890\ttotal: 7m 57s\tremaining: 1m 26s\n",
      "847:\tlearn: 0.0020890\ttotal: 7m 57s\tremaining: 1m 25s\n",
      "848:\tlearn: 0.0020890\ttotal: 7m 57s\tremaining: 1m 24s\n",
      "849:\tlearn: 0.0020890\ttotal: 7m 58s\tremaining: 1m 24s\n",
      "850:\tlearn: 0.0020890\ttotal: 7m 58s\tremaining: 1m 23s\n",
      "851:\tlearn: 0.0020890\ttotal: 7m 58s\tremaining: 1m 23s\n",
      "852:\tlearn: 0.0020890\ttotal: 7m 58s\tremaining: 1m 22s\n",
      "853:\tlearn: 0.0020857\ttotal: 7m 59s\tremaining: 1m 21s\n",
      "854:\tlearn: 0.0020823\ttotal: 8m\tremaining: 1m 21s\n",
      "855:\tlearn: 0.0020823\ttotal: 8m\tremaining: 1m 20s\n",
      "856:\tlearn: 0.0020785\ttotal: 8m 1s\tremaining: 1m 20s\n",
      "857:\tlearn: 0.0020785\ttotal: 8m 1s\tremaining: 1m 19s\n",
      "858:\tlearn: 0.0020785\ttotal: 8m 1s\tremaining: 1m 19s\n",
      "859:\tlearn: 0.0020751\ttotal: 8m 2s\tremaining: 1m 18s\n",
      "860:\tlearn: 0.0020751\ttotal: 8m 2s\tremaining: 1m 17s\n",
      "861:\tlearn: 0.0020751\ttotal: 8m 2s\tremaining: 1m 17s\n",
      "862:\tlearn: 0.0020751\ttotal: 8m 2s\tremaining: 1m 16s\n",
      "863:\tlearn: 0.0020714\ttotal: 8m 3s\tremaining: 1m 16s\n",
      "864:\tlearn: 0.0020714\ttotal: 8m 3s\tremaining: 1m 15s\n",
      "865:\tlearn: 0.0020714\ttotal: 8m 3s\tremaining: 1m 14s\n",
      "866:\tlearn: 0.0020714\ttotal: 8m 4s\tremaining: 1m 14s\n",
      "867:\tlearn: 0.0020714\ttotal: 8m 4s\tremaining: 1m 13s\n",
      "868:\tlearn: 0.0020714\ttotal: 8m 4s\tremaining: 1m 13s\n",
      "869:\tlearn: 0.0020714\ttotal: 8m 4s\tremaining: 1m 12s\n",
      "870:\tlearn: 0.0020679\ttotal: 8m 5s\tremaining: 1m 11s\n",
      "871:\tlearn: 0.0020679\ttotal: 8m 5s\tremaining: 1m 11s\n",
      "872:\tlearn: 0.0020644\ttotal: 8m 6s\tremaining: 1m 10s\n",
      "873:\tlearn: 0.0020611\ttotal: 8m 7s\tremaining: 1m 10s\n",
      "874:\tlearn: 0.0020570\ttotal: 8m 7s\tremaining: 1m 9s\n",
      "875:\tlearn: 0.0020537\ttotal: 8m 8s\tremaining: 1m 9s\n",
      "876:\tlearn: 0.0020499\ttotal: 8m 8s\tremaining: 1m 8s\n",
      "877:\tlearn: 0.0020499\ttotal: 8m 9s\tremaining: 1m 7s\n",
      "878:\tlearn: 0.0020465\ttotal: 8m 9s\tremaining: 1m 7s\n",
      "879:\tlearn: 0.0020465\ttotal: 8m 10s\tremaining: 1m 6s\n",
      "880:\tlearn: 0.0020465\ttotal: 8m 10s\tremaining: 1m 6s\n",
      "881:\tlearn: 0.0020425\ttotal: 8m 10s\tremaining: 1m 5s\n",
      "882:\tlearn: 0.0020425\ttotal: 8m 11s\tremaining: 1m 5s\n",
      "883:\tlearn: 0.0020425\ttotal: 8m 11s\tremaining: 1m 4s\n",
      "884:\tlearn: 0.0020425\ttotal: 8m 11s\tremaining: 1m 3s\n",
      "885:\tlearn: 0.0020425\ttotal: 8m 11s\tremaining: 1m 3s\n",
      "886:\tlearn: 0.0020425\ttotal: 8m 12s\tremaining: 1m 2s\n",
      "887:\tlearn: 0.0020416\ttotal: 8m 12s\tremaining: 1m 2s\n",
      "888:\tlearn: 0.0020416\ttotal: 8m 12s\tremaining: 1m 1s\n",
      "889:\tlearn: 0.0020416\ttotal: 8m 13s\tremaining: 1m\n",
      "890:\tlearn: 0.0020416\ttotal: 8m 13s\tremaining: 1m\n",
      "891:\tlearn: 0.0020416\ttotal: 8m 13s\tremaining: 59.7s\n",
      "892:\tlearn: 0.0020416\ttotal: 8m 13s\tremaining: 59.2s\n",
      "893:\tlearn: 0.0020379\ttotal: 8m 14s\tremaining: 58.6s\n",
      "894:\tlearn: 0.0020379\ttotal: 8m 14s\tremaining: 58s\n",
      "895:\tlearn: 0.0020379\ttotal: 8m 14s\tremaining: 57.4s\n",
      "896:\tlearn: 0.0020342\ttotal: 8m 15s\tremaining: 56.9s\n",
      "897:\tlearn: 0.0020342\ttotal: 8m 15s\tremaining: 56.3s\n",
      "898:\tlearn: 0.0020306\ttotal: 8m 16s\tremaining: 55.8s\n",
      "899:\tlearn: 0.0020270\ttotal: 8m 17s\tremaining: 55.2s\n",
      "900:\tlearn: 0.0020270\ttotal: 8m 17s\tremaining: 54.6s\n",
      "901:\tlearn: 0.0020270\ttotal: 8m 17s\tremaining: 54.1s\n",
      "902:\tlearn: 0.0020236\ttotal: 8m 18s\tremaining: 53.5s\n",
      "903:\tlearn: 0.0020236\ttotal: 8m 18s\tremaining: 52.9s\n",
      "904:\tlearn: 0.0020236\ttotal: 8m 18s\tremaining: 52.3s\n",
      "905:\tlearn: 0.0020236\ttotal: 8m 18s\tremaining: 51.8s\n",
      "906:\tlearn: 0.0020236\ttotal: 8m 19s\tremaining: 51.2s\n",
      "907:\tlearn: 0.0020236\ttotal: 8m 19s\tremaining: 50.6s\n",
      "908:\tlearn: 0.0020236\ttotal: 8m 19s\tremaining: 50s\n",
      "909:\tlearn: 0.0020202\ttotal: 8m 20s\tremaining: 49.5s\n",
      "910:\tlearn: 0.0020202\ttotal: 8m 20s\tremaining: 48.9s\n",
      "911:\tlearn: 0.0020170\ttotal: 8m 21s\tremaining: 48.4s\n",
      "912:\tlearn: 0.0020169\ttotal: 8m 21s\tremaining: 47.8s\n",
      "913:\tlearn: 0.0020136\ttotal: 8m 22s\tremaining: 47.2s\n",
      "914:\tlearn: 0.0020103\ttotal: 8m 22s\tremaining: 46.7s\n",
      "915:\tlearn: 0.0020070\ttotal: 8m 23s\tremaining: 46.2s\n",
      "916:\tlearn: 0.0020038\ttotal: 8m 23s\tremaining: 45.6s\n",
      "917:\tlearn: 0.0020008\ttotal: 8m 24s\tremaining: 45.1s\n",
      "918:\tlearn: 0.0020008\ttotal: 8m 24s\tremaining: 44.5s\n",
      "919:\tlearn: 0.0020008\ttotal: 8m 25s\tremaining: 43.9s\n",
      "920:\tlearn: 0.0019976\ttotal: 8m 25s\tremaining: 43.4s\n",
      "921:\tlearn: 0.0019976\ttotal: 8m 25s\tremaining: 42.8s\n",
      "922:\tlearn: 0.0019976\ttotal: 8m 26s\tremaining: 42.2s\n",
      "923:\tlearn: 0.0019945\ttotal: 8m 26s\tremaining: 41.7s\n",
      "924:\tlearn: 0.0019945\ttotal: 8m 27s\tremaining: 41.1s\n",
      "925:\tlearn: 0.0019945\ttotal: 8m 27s\tremaining: 40.5s\n",
      "926:\tlearn: 0.0019945\ttotal: 8m 27s\tremaining: 40s\n",
      "927:\tlearn: 0.0019912\ttotal: 8m 28s\tremaining: 39.4s\n",
      "928:\tlearn: 0.0019912\ttotal: 8m 28s\tremaining: 38.9s\n",
      "929:\tlearn: 0.0019878\ttotal: 8m 29s\tremaining: 38.3s\n",
      "930:\tlearn: 0.0019846\ttotal: 8m 29s\tremaining: 37.8s\n",
      "931:\tlearn: 0.0019846\ttotal: 8m 29s\tremaining: 37.2s\n",
      "932:\tlearn: 0.0019846\ttotal: 8m 30s\tremaining: 36.6s\n",
      "933:\tlearn: 0.0019846\ttotal: 8m 30s\tremaining: 36.1s\n",
      "934:\tlearn: 0.0019846\ttotal: 8m 30s\tremaining: 35.5s\n",
      "935:\tlearn: 0.0019846\ttotal: 8m 30s\tremaining: 34.9s\n",
      "936:\tlearn: 0.0019846\ttotal: 8m 31s\tremaining: 34.4s\n",
      "937:\tlearn: 0.0019846\ttotal: 8m 31s\tremaining: 33.8s\n",
      "938:\tlearn: 0.0019846\ttotal: 8m 31s\tremaining: 33.2s\n",
      "939:\tlearn: 0.0019846\ttotal: 8m 31s\tremaining: 32.7s\n",
      "940:\tlearn: 0.0019813\ttotal: 8m 32s\tremaining: 32.1s\n",
      "941:\tlearn: 0.0019813\ttotal: 8m 32s\tremaining: 31.6s\n",
      "942:\tlearn: 0.0019813\ttotal: 8m 32s\tremaining: 31s\n",
      "943:\tlearn: 0.0019813\ttotal: 8m 33s\tremaining: 30.4s\n",
      "944:\tlearn: 0.0019813\ttotal: 8m 33s\tremaining: 29.9s\n",
      "945:\tlearn: 0.0019780\ttotal: 8m 33s\tremaining: 29.3s\n",
      "946:\tlearn: 0.0019780\ttotal: 8m 34s\tremaining: 28.8s\n",
      "947:\tlearn: 0.0019746\ttotal: 8m 34s\tremaining: 28.2s\n",
      "948:\tlearn: 0.0019746\ttotal: 8m 35s\tremaining: 27.7s\n",
      "949:\tlearn: 0.0019746\ttotal: 8m 35s\tremaining: 27.1s\n",
      "950:\tlearn: 0.0019746\ttotal: 8m 35s\tremaining: 26.6s\n",
      "951:\tlearn: 0.0019746\ttotal: 8m 35s\tremaining: 26s\n",
      "952:\tlearn: 0.0019746\ttotal: 8m 36s\tremaining: 25.4s\n",
      "953:\tlearn: 0.0019746\ttotal: 8m 36s\tremaining: 24.9s\n",
      "954:\tlearn: 0.0019714\ttotal: 8m 36s\tremaining: 24.4s\n",
      "955:\tlearn: 0.0019714\ttotal: 8m 37s\tremaining: 23.8s\n",
      "956:\tlearn: 0.0019714\ttotal: 8m 37s\tremaining: 23.2s\n",
      "957:\tlearn: 0.0019714\ttotal: 8m 37s\tremaining: 22.7s\n",
      "958:\tlearn: 0.0019714\ttotal: 8m 37s\tremaining: 22.1s\n",
      "959:\tlearn: 0.0019683\ttotal: 8m 38s\tremaining: 21.6s\n",
      "960:\tlearn: 0.0019683\ttotal: 8m 38s\tremaining: 21.1s\n",
      "961:\tlearn: 0.0019654\ttotal: 8m 39s\tremaining: 20.5s\n",
      "962:\tlearn: 0.0019654\ttotal: 8m 39s\tremaining: 20s\n",
      "963:\tlearn: 0.0019654\ttotal: 8m 39s\tremaining: 19.4s\n",
      "964:\tlearn: 0.0019654\ttotal: 8m 40s\tremaining: 18.9s\n",
      "965:\tlearn: 0.0019654\ttotal: 8m 40s\tremaining: 18.3s\n",
      "966:\tlearn: 0.0019654\ttotal: 8m 40s\tremaining: 17.8s\n",
      "967:\tlearn: 0.0019622\ttotal: 8m 41s\tremaining: 17.2s\n",
      "968:\tlearn: 0.0019622\ttotal: 8m 41s\tremaining: 16.7s\n",
      "969:\tlearn: 0.0019622\ttotal: 8m 41s\tremaining: 16.1s\n",
      "970:\tlearn: 0.0019591\ttotal: 8m 42s\tremaining: 15.6s\n",
      "971:\tlearn: 0.0019591\ttotal: 8m 42s\tremaining: 15.1s\n",
      "972:\tlearn: 0.0019557\ttotal: 8m 43s\tremaining: 14.5s\n",
      "973:\tlearn: 0.0019557\ttotal: 8m 43s\tremaining: 14s\n",
      "974:\tlearn: 0.0019557\ttotal: 8m 43s\tremaining: 13.4s\n",
      "975:\tlearn: 0.0019557\ttotal: 8m 43s\tremaining: 12.9s\n",
      "976:\tlearn: 0.0019557\ttotal: 8m 44s\tremaining: 12.3s\n",
      "977:\tlearn: 0.0019557\ttotal: 8m 44s\tremaining: 11.8s\n",
      "978:\tlearn: 0.0019557\ttotal: 8m 44s\tremaining: 11.3s\n",
      "979:\tlearn: 0.0019526\ttotal: 8m 45s\tremaining: 10.7s\n",
      "980:\tlearn: 0.0019526\ttotal: 8m 45s\tremaining: 10.2s\n",
      "981:\tlearn: 0.0019495\ttotal: 8m 46s\tremaining: 9.65s\n",
      "982:\tlearn: 0.0019495\ttotal: 8m 46s\tremaining: 9.11s\n",
      "983:\tlearn: 0.0019464\ttotal: 8m 47s\tremaining: 8.57s\n",
      "984:\tlearn: 0.0019464\ttotal: 8m 47s\tremaining: 8.03s\n",
      "985:\tlearn: 0.0019464\ttotal: 8m 47s\tremaining: 7.49s\n",
      "986:\tlearn: 0.0019464\ttotal: 8m 48s\tremaining: 6.95s\n",
      "987:\tlearn: 0.0019464\ttotal: 8m 48s\tremaining: 6.42s\n",
      "988:\tlearn: 0.0019089\ttotal: 8m 48s\tremaining: 5.88s\n",
      "989:\tlearn: 0.0019059\ttotal: 8m 49s\tremaining: 5.35s\n",
      "990:\tlearn: 0.0019030\ttotal: 8m 50s\tremaining: 4.81s\n",
      "991:\tlearn: 0.0019000\ttotal: 8m 50s\tremaining: 4.28s\n",
      "992:\tlearn: 0.0018972\ttotal: 8m 51s\tremaining: 3.75s\n",
      "993:\tlearn: 0.0018944\ttotal: 8m 52s\tremaining: 3.21s\n",
      "994:\tlearn: 0.0018944\ttotal: 8m 52s\tremaining: 2.67s\n",
      "995:\tlearn: 0.0018944\ttotal: 8m 52s\tremaining: 2.14s\n",
      "996:\tlearn: 0.0018914\ttotal: 8m 53s\tremaining: 1.6s\n",
      "997:\tlearn: 0.0018888\ttotal: 8m 53s\tremaining: 1.07s\n",
      "998:\tlearn: 0.0018854\ttotal: 8m 54s\tremaining: 535ms\n",
      "999:\tlearn: 0.0018854\ttotal: 8m 54s\tremaining: 0us\n",
      "Accuracy CatBoost: 0.9434908898864537\n",
      "Confusion Matrix:\n",
      "[[1760  134]\n",
      " [  80 1813]]\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from catboost import CatBoostClassifier #pip install catboost\n",
    "\n",
    "params = OrderedDict([('bagging_temperature', 1.0), ('border_count', 148), ('depth', 13), ('iterations', 976), ('l2_leaf_reg', 30), ('learning_rate', 0.02389354323083735), ('random_strength', 1e-09), ('scale_pos_weight', 0.9510644977326121)])\n",
    "params['iterations'] = 1000\n",
    "\n",
    "params2 = OrderedDict([('bagging_temperature', 1.0), ('border_count', 86), ('depth', 16), ('iterations', 868), ('l2_leaf_reg', 2), ('learning_rate', 0.06598200301644333), ('random_strength', 10.0), ('scale_pos_weight', 0.8384915263866678)])\n",
    "params2['iterations'] = 1000\n",
    "cb = CatBoostClassifier(**params2, verbose=True)\n",
    "cb.fit(X_train_dummy, y_train_dummy['final_result'])\n",
    "\n",
    "y_pred = cb.predict(X_test_dummy)\n",
    "acc = accuracy_score(y_test_dummy['final_result'], y_pred)\n",
    "print('Accuracy CatBoost:', acc)\n",
    "cm = confusion_matrix(y_test_dummy['final_result'], y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Random Forest: 0.9390018484288355\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_dummy, y_train_dummy['final_result'])\n",
    "\n",
    "y_pred = rf.predict(X_test_dummy)\n",
    "acc = accuracy_score(y_test_dummy['final_result'], y_pred)\n",
    "print('Accuracy Random Forest:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "categories = ['code_module', 'code_presentation', 'gender', 'region']\n",
    "numerical_features = [col for col in X_train_ord.columns if col not in categories]\n",
    "\n",
    "X_train_dummy = scaler.fit_transform(X_train_dummy)\n",
    "X_test_dummy = scaler.fit_transform(X_test_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_result</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19901</th>\n",
       "      <td>0</td>\n",
       "      <td>77.718532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14299</th>\n",
       "      <td>1</td>\n",
       "      <td>84.191919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25453</th>\n",
       "      <td>0</td>\n",
       "      <td>84.567035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6573</th>\n",
       "      <td>1</td>\n",
       "      <td>85.348315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11571</th>\n",
       "      <td>0</td>\n",
       "      <td>67.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       final_result      score\n",
       "19901             0  77.718532\n",
       "14299             1  84.191919\n",
       "25453             0  84.567035\n",
       "6573              1  85.348315\n",
       "11571             0  67.857143"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 20:43:50.365305: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-03-26 20:43:51.439909: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-03-26 20:43:51.439995: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2024-03-26 20:43:51.443593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:ca:00.0 name: NVIDIA A40 computeCapability: 8.6\n",
      "coreClock: 1.74GHz coreCount: 84 deviceMemorySize: 44.38GiB deviceMemoryBandwidth: 648.29GiB/s\n",
      "2024-03-26 20:43:51.443622: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-03-26 20:43:51.446057: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2024-03-26 20:43:51.446116: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2024-03-26 20:43:51.447162: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2024-03-26 20:43:51.447431: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2024-03-26 20:43:51.450003: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-03-26 20:43:51.450613: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-03-26 20:43:51.450768: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2024-03-26 20:43:51.455450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2024-03-26 20:43:51.456464: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-26 20:43:51.463731: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-03-26 20:43:51.466778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:ca:00.0 name: NVIDIA A40 computeCapability: 8.6\n",
      "coreClock: 1.74GHz coreCount: 84 deviceMemorySize: 44.38GiB deviceMemoryBandwidth: 648.29GiB/s\n",
      "2024-03-26 20:43:51.466878: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-03-26 20:43:51.466945: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2024-03-26 20:43:51.466999: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2024-03-26 20:43:51.467052: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2024-03-26 20:43:51.467105: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2024-03-26 20:43:51.467157: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-03-26 20:43:51.467209: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-03-26 20:43:51.467262: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2024-03-26 20:43:51.472009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2024-03-26 20:43:51.472057: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-03-26 20:43:51.979286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-03-26 20:43:51.979330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2024-03-26 20:43:51.979339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2024-03-26 20:43:51.981809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 42329 MB memory) -> physical GPU (device: 0, name: NVIDIA A40, pci bus id: 0000:ca:00.0, compute capability: 8.6)\n",
      "2024-03-26 20:43:52.687757: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2024-03-26 20:43:52.705781: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 20:43:53.458900: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38/379 [==>...........................] - ETA: 1s - loss: 3592.8462 - accuracy: 9.2491e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 20:43:54.126222: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2024-03-26 20:43:54.132094: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 4s 6ms/step - loss: 2901.7570 - accuracy: 3.8874e-04 - val_loss: 893.8521 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 893.85205, saving model to model.h5\n",
      "Epoch 2/15\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 511.0065 - accuracy: 0.0000e+00 - val_loss: 295.4466 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss improved from 893.85205 to 295.44662, saving model to model.h5\n",
      "Epoch 3/15\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 307.8645 - accuracy: 0.0000e+00 - val_loss: 276.8907 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_loss improved from 295.44662 to 276.89072, saving model to model.h5\n",
      "Epoch 4/15\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 280.9080 - accuracy: 0.0000e+00 - val_loss: 239.7257 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_loss improved from 276.89072 to 239.72572, saving model to model.h5\n",
      "Epoch 5/15\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 262.6265 - accuracy: 0.0000e+00 - val_loss: 231.3505 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_loss improved from 239.72572 to 231.35051, saving model to model.h5\n",
      "Epoch 6/15\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 253.4206 - accuracy: 0.0000e+00 - val_loss: 283.3188 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 231.35051\n",
      "Epoch 7/15\n",
      "379/379 [==============================] - 2s 4ms/step - loss: 240.2484 - accuracy: 0.0000e+00 - val_loss: 236.3077 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 231.35051\n",
      "Epoch 8/15\n",
      "379/379 [==============================] - 2s 4ms/step - loss: 235.6435 - accuracy: 0.0000e+00 - val_loss: 210.5917 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_loss improved from 231.35051 to 210.59174, saving model to model.h5\n",
      "Epoch 9/15\n",
      "379/379 [==============================] - 2s 4ms/step - loss: 222.2418 - accuracy: 0.0000e+00 - val_loss: 216.7896 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 210.59174\n",
      "Epoch 10/15\n",
      "379/379 [==============================] - 2s 4ms/step - loss: 230.9379 - accuracy: 0.0000e+00 - val_loss: 205.4115 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_loss improved from 210.59174 to 205.41153, saving model to model.h5\n",
      "Epoch 11/15\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 219.1708 - accuracy: 0.0000e+00 - val_loss: 237.7905 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 205.41153\n",
      "Epoch 12/15\n",
      "379/379 [==============================] - 2s 4ms/step - loss: 221.9532 - accuracy: 0.0000e+00 - val_loss: 205.5180 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 205.41153\n",
      "Epoch 13/15\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 214.6097 - accuracy: 0.0000e+00 - val_loss: 188.6598 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00013: val_loss improved from 205.41153 to 188.65977, saving model to model.h5\n",
      "Epoch 14/15\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 206.9584 - accuracy: 0.0000e+00 - val_loss: 190.2295 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 188.65977\n",
      "Epoch 15/15\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 206.5772 - accuracy: 0.0000e+00 - val_loss: 263.2967 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 188.65977\n",
      "predicting\n",
      "misclassifying\n",
      "Epoch 1/30\n",
      "131/131 [==============================] - 1s 6ms/step - loss: 284.5562 - accuracy: 0.0000e+00 - val_loss: 173.2914 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 173.29143, saving model to model2.h5\n",
      "Epoch 2/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 278.9064 - accuracy: 0.0000e+00 - val_loss: 207.9815 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 173.29143\n",
      "Epoch 3/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 281.0049 - accuracy: 0.0000e+00 - val_loss: 177.0601 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 173.29143\n",
      "Epoch 4/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 262.3549 - accuracy: 0.0000e+00 - val_loss: 181.1154 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 173.29143\n",
      "Epoch 5/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 263.5966 - accuracy: 0.0000e+00 - val_loss: 214.5796 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 173.29143\n",
      "Epoch 6/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 263.5634 - accuracy: 0.0000e+00 - val_loss: 158.7956 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_loss improved from 173.29143 to 158.79561, saving model to model2.h5\n",
      "Epoch 7/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 263.4543 - accuracy: 0.0000e+00 - val_loss: 184.8713 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 158.79561\n",
      "Epoch 8/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 261.2303 - accuracy: 0.0000e+00 - val_loss: 170.3486 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 158.79561\n",
      "Epoch 9/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 264.0577 - accuracy: 0.0000e+00 - val_loss: 153.2989 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_loss improved from 158.79561 to 153.29889, saving model to model2.h5\n",
      "Epoch 10/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 251.2418 - accuracy: 0.0000e+00 - val_loss: 229.8128 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 153.29889\n",
      "Epoch 11/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 246.9354 - accuracy: 0.0000e+00 - val_loss: 339.1785 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 153.29889\n",
      "Epoch 12/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 257.2326 - accuracy: 0.0000e+00 - val_loss: 194.0614 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 153.29889\n",
      "Epoch 13/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 245.3489 - accuracy: 0.0000e+00 - val_loss: 202.6293 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 153.29889\n",
      "Epoch 14/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 249.4195 - accuracy: 0.0000e+00 - val_loss: 262.8705 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 153.29889\n",
      "Epoch 15/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 246.6704 - accuracy: 0.0000e+00 - val_loss: 305.9364 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 153.29889\n",
      "Epoch 16/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 245.8507 - accuracy: 0.0000e+00 - val_loss: 243.0269 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 153.29889\n",
      "Epoch 17/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 252.6163 - accuracy: 0.0000e+00 - val_loss: 255.9965 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 153.29889\n",
      "Epoch 18/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 242.8769 - accuracy: 0.0000e+00 - val_loss: 191.2562 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 153.29889\n",
      "Epoch 19/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 239.6790 - accuracy: 0.0000e+00 - val_loss: 194.4855 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 153.29889\n",
      "Epoch 20/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 246.7408 - accuracy: 0.0000e+00 - val_loss: 372.1851 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 153.29889\n",
      "Epoch 21/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 255.8775 - accuracy: 0.0000e+00 - val_loss: 251.3860 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 153.29889\n",
      "Epoch 22/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 239.7829 - accuracy: 0.0000e+00 - val_loss: 154.9549 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 153.29889\n",
      "Epoch 23/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 240.3920 - accuracy: 0.0000e+00 - val_loss: 174.6939 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 153.29889\n",
      "Epoch 24/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 239.9181 - accuracy: 0.0000e+00 - val_loss: 282.1877 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 153.29889\n",
      "Epoch 25/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 254.7216 - accuracy: 0.0000e+00 - val_loss: 221.1675 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 153.29889\n",
      "Epoch 26/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 241.0690 - accuracy: 0.0000e+00 - val_loss: 137.1689 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00026: val_loss improved from 153.29889 to 137.16888, saving model to model2.h5\n",
      "Epoch 27/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 246.5137 - accuracy: 0.0000e+00 - val_loss: 202.0037 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 137.16888\n",
      "Epoch 28/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 255.3139 - accuracy: 0.0000e+00 - val_loss: 253.0789 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 137.16888\n",
      "Epoch 29/30\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 238.5589 - accuracy: 0.0000e+00 - val_loss: 195.9720 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 137.16888\n",
      "Epoch 30/30\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 248.4754 - accuracy: 0.0000e+00 - val_loss: 215.5795 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 137.16888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1694460a30>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout, Input, Dense, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train_dummy_np = X_train_dummy.astype('float32')\n",
    "y_train_dummy_np = y_train_dummy.drop(columns=['final_result']).astype('float32')\n",
    "\n",
    "input_layer = Input(shape=(X_train_dummy.shape[1],))\n",
    "\n",
    "dense = Dense(128, kernel_initializer=tf.keras.initializers.HeNormal(), kernel_regularizer=l2(0.01))(input_layer)\n",
    "dense = BatchNormalization()(dense)\n",
    "dense = tf.keras.activations.relu(dense)\n",
    "dense = Dropout(0.2)(dense)\n",
    "\n",
    "dense = Dense(128, kernel_initializer=tf.keras.initializers.HeNormal(), kernel_regularizer=l2(0.01))(dense)\n",
    "dense = BatchNormalization()(dense)\n",
    "dense = tf.keras.activations.relu(dense)\n",
    "dense = Dropout(0.2)(dense)\n",
    "\n",
    "dense = Dense(128, kernel_initializer=tf.keras.initializers.HeNormal(), kernel_regularizer=l2(0.01))(dense)\n",
    "dense = BatchNormalization()(dense)\n",
    "dense = tf.keras.activations.relu(dense)\n",
    "dense = Dropout(0.2)(dense)\n",
    "\n",
    "output = Dense(1, activation='linear')(dense)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "model2 = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='model.h5',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "checkpoint2 = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='model2.h5',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model2.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "hist = model.fit(X_train_dummy_np, y_train_dummy_np, epochs=15, batch_size=32, validation_split=0.2, callbacks=[checkpoint])\n",
    "\n",
    "#misclassified from the first\n",
    "print(\"predicting\")\n",
    "preds1 = model.predict(X_train_dummy_np)\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_dummy['final_result'], preds1)\n",
    "best_threshold = thresholds[np.argmax(precisions + recalls)]\n",
    "print(\"misclassifying\")\n",
    "\n",
    "predictions_classified = [1 if y >= best_threshold else 0 for y in preds1.flatten()]\n",
    "# y_train_dummy_binary = [1 if y >= 50 else 0 for y in y_train_dummy['score']]\n",
    "np.random.seed(0)\n",
    "misclassified = np.array(predictions_classified) != np.array(y_train_dummy['final_result'])\n",
    "misclassified_indices = np.where(misclassified)[0]\n",
    "correct_indices = np.where(~misclassified)[0]\n",
    "random_correct_indices = np.random.choice (correct_indices, size=int(len(misclassified_indices)*2), replace=False)\n",
    "training_indices = np.concatenate([misclassified_indices, random_correct_indices])\n",
    "X_train2 = X_train_dummy_np.iloc[training_indices]\n",
    "y_train2 = y_train_dummy_np.iloc[training_indices]\n",
    "\n",
    "model2.fit(X_train2, y_train2, epochs=30, batch_size=32, validation_split=0.2, callbacks=[checkpoint2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 0's: 1888\n",
      "Number of 1's: 1899\n",
      "SMOTE\n",
      "Average score where final_result = 1: 74.90150212396232\n",
      "Average score where final_result = 0: 39.049781113585574\n",
      "Number of entries where final_result = 1 and score < 50: 0\n",
      "Number of entries where final_result = 0 and score > 50: 0\n",
      "REG\n",
      "Average score where final_result = 1: 74.90150212396232\n",
      "Average score where final_result = 0: 38.80145189638934\n",
      "Number of entries where final_result = 1 and score < 50: 0\n",
      "Number of entries where final_result = 0 and score > 50: 0\n"
     ]
    }
   ],
   "source": [
    "y_test_dummy['final_result'].value_counts()\n",
    "test_classified = [0 if y < 50 else 1 for y in y_test_dummy['score'].astype('float32')]\n",
    "num_zeros = test_classified.count(0)\n",
    "num_ones = test_classified.count(1)\n",
    "\n",
    "print(f\"Number of 0's: {num_zeros}\")\n",
    "print(f\"Number of 1's: {num_ones}\")\n",
    "\n",
    "print('SMOTE')\n",
    "average_score = y_dummy_smote[y_dummy_smote['final_result'] == 1]['score'].mean()\n",
    "print(f\"Average score where final_result = 1: {average_score}\")\n",
    "\n",
    "average_score = y_dummy_smote[y_dummy_smote['final_result'] == 0]['score'].mean()\n",
    "print(f\"Average score where final_result = 0: {average_score}\")\n",
    "\n",
    "num_entries = len(y_dummy_smote[(y_dummy_smote['final_result'] == 1) & (y_dummy_smote['score'] < 50)])\n",
    "print(f\"Number of entries where final_result = 1 and score < 50: {num_entries}\")\n",
    "\n",
    "num_entries = len(y_dummy_smote[(y_dummy_smote['final_result'] == 0) & (y_dummy_smote['score'] > 50)])\n",
    "print(f\"Number of entries where final_result = 0 and score > 50: {num_entries}\")\n",
    "\n",
    "print('REG')\n",
    "average_score = y_dummy[y_dummy['final_result'] == 1]['score'].mean()\n",
    "print(f\"Average score where final_result = 1: {average_score}\")\n",
    "\n",
    "average_score = y_dummy[y_dummy['final_result'] == 0]['score'].mean()\n",
    "print(f\"Average score where final_result = 0: {average_score}\")\n",
    "\n",
    "num_entries = len(y_dummy[(y_dummy['final_result'] == 1) & (y_dummy['score'] < 50)])\n",
    "print(f\"Number of entries where final_result = 1 and score < 50: {num_entries}\")\n",
    "\n",
    "num_entries = len(y_dummy[(y_dummy['final_result'] == 0) & (y_dummy['score'] > 50)])\n",
    "print(f\"Number of entries where final_result = 0 and score > 50: {num_entries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold 55.72512\n",
      "3787\n",
      "acc1 0.8803802482175864\n",
      "Confusion Matrix1:\n",
      "[[1527  367]\n",
      " [  86 1807]]\n",
      "best threshold 55.455837\n",
      "Accuracy 0.9094269870609981\n",
      "Confusion Matrix:\n",
      "[[1607  287]\n",
      " [  56 1837]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "best_model1 = load_model('model.h5')\n",
    "best_model2 = load_model('model2.h5')\n",
    "\n",
    "X_test_dummy_np = X_test_dummy.astype('float32')\n",
    "\n",
    "preds1 = best_model1.predict(X_test_dummy_np)\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test_dummy['final_result'], preds1)\n",
    "best_threshold = thresholds[np.argmax(precisions + recalls)]\n",
    "\n",
    "print('best threshold', best_threshold)\n",
    "\n",
    "predictions_classified1 = []\n",
    "for i, pred in enumerate(preds1):\n",
    "#     print(i, pred)\n",
    "    if (pred >= best_threshold):\n",
    "        predictions_classified1.append(1)\n",
    "    else:\n",
    "        predictions_classified1.append(0)\n",
    "print(len(predictions_classified1))\n",
    "\n",
    "# y_test_dummy_binary = [1 if y >= 60 else 0 for y in y_test_dummy['score']]\n",
    "# predictions_classified1 = [1 if y >= 60 else 0 for y in preds1]\n",
    "\n",
    "acc = accuracy_score(y_test_dummy['final_result'], predictions_classified1)\n",
    "print(\"acc1\", acc)\n",
    "cm = confusion_matrix(y_test_dummy['final_result'], predictions_classified1)\n",
    "\n",
    "print(\"Confusion Matrix1:\")\n",
    "print(cm)\n",
    "positive_preds = np.array(predictions_classified1) == 1\n",
    "\n",
    "preds2 = best_model2.predict(X_test_dummy_np)#[positive_preds])\n",
    "# preds2 = best_model2.predict(X_test_dummy_np)\n",
    "average_preds = (preds1*0.45) + (preds2*0.55)\n",
    "\n",
    "\n",
    "\n",
    "final_preds = np.array(preds1.copy())\n",
    "final_preds = average_preds#[positive_preds] = average_preds\n",
    "# final_preds = np.array(preds.copy())\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test_dummy['final_result'], final_preds)\n",
    "best_threshold = thresholds[np.argmax(precisions + recalls)]\n",
    "\n",
    "print('best threshold', best_threshold)\n",
    "final_preds_classified = [1 if y >= best_threshold else 0 for y in final_preds.flatten()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test_dummy['final_result'], final_preds_classified)\n",
    "print(\"Accuracy\", acc)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test_dummy['final_result'], final_preds_classified)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN, TAKES FOREVER, OPTIMAL PARAMS PRINTED BELOW\n",
    "\n",
    "from skopt import BayesSearchCV #pip install scikit-optimize\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from joblib import parallel_backend\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "def create_model(neurons, dropout_rate, num_layers, learning_rate):\n",
    "    input_layer = Input(shape=(X_train_dummy.shape[1],))\n",
    "\n",
    "    dense = Dense(neurons, kernel_initializer=tf.keras.initializers.HeNormal())(input_layer)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = tf.keras.activations.relu(dense)\n",
    "    dense = Dropout(dropout_rate)(dense)\n",
    "    \n",
    "    for i in range(num_layers - 1):\n",
    "        dense = Dense(neurons, kernel_initializer=tf.keras.initializers.HeNormal())(dense)\n",
    "        dense = BatchNormalization()(dense)\n",
    "        dense = tf.keras.activations.relu(dense)\n",
    "        dense = Dropout(dropout_rate)(dense)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "wrapped_create_model = KerasClassifier(build_fn=create_model, verbose=1, epochs=25)\n",
    "\n",
    "param_space = {\n",
    "    'neurons': (8, 512),\n",
    "    'dropout_rate': (0.1, 0.5),\n",
    "    'num_layers': (1, 5),\n",
    "    'learning_rate': (0.0001, 0.1)\n",
    "}\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    wrapped_create_model,\n",
    "    param_space,\n",
    "    n_iter=50,\n",
    "    cv=3,\n",
    "    n_jobs=1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "opt.fit(X_train_dummy, y_train_dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = opt.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_parameters = opt.best_params_\n",
    "\n",
    "optimal_model = create_model(\n",
    "    neurons=optimal_parameters['neurons'],\n",
    "    dropout_rate=optimal_parameters['dropout_rate'],\n",
    "    num_layers=optimal_parameters['num_layers'],\n",
    "    learning_rate=optimal_parameters['learning_rate']\n",
    ")\n",
    "\n",
    "history = optimal_model.fit(X_train_dummy, y_train_dummy, epochs=30, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = optimal_model.evaluate(X_test_dummy, y_test_dummy)\n",
    "print(\"Accuracy (tuned model):\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
