{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_module</th>\n",
       "      <th>code_presentation</th>\n",
       "      <th>id_student</th>\n",
       "      <th>gender</th>\n",
       "      <th>region</th>\n",
       "      <th>highest_education</th>\n",
       "      <th>imd_band</th>\n",
       "      <th>age_band</th>\n",
       "      <th>num_of_prev_attempts</th>\n",
       "      <th>studied_credits</th>\n",
       "      <th>disability</th>\n",
       "      <th>final_result</th>\n",
       "      <th>score</th>\n",
       "      <th>grade</th>\n",
       "      <th>studied_credits_binned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAA</td>\n",
       "      <td>2013J</td>\n",
       "      <td>11391</td>\n",
       "      <td>M</td>\n",
       "      <td>East Anglian Region</td>\n",
       "      <td>HE Qualification</td>\n",
       "      <td>90-100%</td>\n",
       "      <td>55&lt;=</td>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>N</td>\n",
       "      <td>Pass</td>\n",
       "      <td>82.4</td>\n",
       "      <td>A-</td>\n",
       "      <td>201+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAA</td>\n",
       "      <td>2013J</td>\n",
       "      <td>28400</td>\n",
       "      <td>F</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>HE Qualification</td>\n",
       "      <td>20-30%</td>\n",
       "      <td>35-55</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>N</td>\n",
       "      <td>Pass</td>\n",
       "      <td>65.4</td>\n",
       "      <td>C</td>\n",
       "      <td>30-60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAA</td>\n",
       "      <td>2013J</td>\n",
       "      <td>31604</td>\n",
       "      <td>F</td>\n",
       "      <td>South East Region</td>\n",
       "      <td>A Level or Equivalent</td>\n",
       "      <td>50-60%</td>\n",
       "      <td>35-55</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>N</td>\n",
       "      <td>Pass</td>\n",
       "      <td>76.3</td>\n",
       "      <td>B</td>\n",
       "      <td>30-60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAA</td>\n",
       "      <td>2013J</td>\n",
       "      <td>32885</td>\n",
       "      <td>F</td>\n",
       "      <td>West Midlands Region</td>\n",
       "      <td>Lower Than A Level</td>\n",
       "      <td>50-60%</td>\n",
       "      <td>0-35</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>N</td>\n",
       "      <td>Pass</td>\n",
       "      <td>55.0</td>\n",
       "      <td>D</td>\n",
       "      <td>30-60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAA</td>\n",
       "      <td>2013J</td>\n",
       "      <td>38053</td>\n",
       "      <td>M</td>\n",
       "      <td>Wales</td>\n",
       "      <td>A Level or Equivalent</td>\n",
       "      <td>80-90%</td>\n",
       "      <td>35-55</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>N</td>\n",
       "      <td>Pass</td>\n",
       "      <td>66.9</td>\n",
       "      <td>C</td>\n",
       "      <td>30-60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code_module code_presentation  id_student gender                region  \\\n",
       "0         AAA             2013J       11391      M   East Anglian Region   \n",
       "1         AAA             2013J       28400      F              Scotland   \n",
       "2         AAA             2013J       31604      F     South East Region   \n",
       "3         AAA             2013J       32885      F  West Midlands Region   \n",
       "4         AAA             2013J       38053      M                 Wales   \n",
       "\n",
       "       highest_education imd_band age_band  num_of_prev_attempts  \\\n",
       "0       HE Qualification  90-100%     55<=                     0   \n",
       "1       HE Qualification   20-30%    35-55                     0   \n",
       "2  A Level or Equivalent   50-60%    35-55                     0   \n",
       "3     Lower Than A Level   50-60%     0-35                     0   \n",
       "4  A Level or Equivalent   80-90%    35-55                     0   \n",
       "\n",
       "   studied_credits disability final_result  score grade studied_credits_binned  \n",
       "0              240          N         Pass   82.4    A-                   201+  \n",
       "1               60          N         Pass   65.4     C                  30-60  \n",
       "2               60          N         Pass   76.3     B                  30-60  \n",
       "3               60          N         Pass   55.0     D                  30-60  \n",
       "4               60          N         Pass   66.9     C                  30-60  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/student_info_cleaned.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['final_result'].value_counts()\n",
    "#drop withdrawn students\n",
    "data = data[data['final_result'] != 'Withdrawn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#order highest_education, imd_band, age_band, disability, studied_credits_binned, final_result\n",
    "highest_education = {\n",
    "    'No Formal quals': 0,\n",
    "    'Lower Than A Level': 1,\n",
    "    'A Level or Equivalent': 2,\n",
    "    'HE Qualification': 3,\n",
    "    'Post Graduate Qualification': 4\n",
    "}\n",
    "\n",
    "imd_band = {\n",
    "    np.nan: -1,\n",
    "    '0-10%': 0,\n",
    "    '10-20': 1,\n",
    "    '20-30%': 2,\n",
    "    '30-40%': 3,\n",
    "    '40-50%': 4,\n",
    "    '50-60%': 5,\n",
    "    '60-70%': 6,\n",
    "    '70-80%': 7,\n",
    "    '80-90%': 8,\n",
    "    '90-100%': 9\n",
    "}\n",
    "\n",
    "age_band = {\n",
    "    '0-35': 0,\n",
    "    '35-55': 1,\n",
    "    '55<=': 2\n",
    "}\n",
    "\n",
    "disability = {\n",
    "    'N': 0,\n",
    "    'Y': 1\n",
    "}\n",
    "\n",
    "studied_credits_binned = {\n",
    "    '30-60': 0,\n",
    "    '61-100': 1,\n",
    "    '101-200': 2,\n",
    "    '201+': 3\n",
    "}\n",
    "\n",
    "final_result = {\n",
    "    'Fail': 0,\n",
    "    'Pass': 1,\n",
    "    'Distinction': 1,\n",
    "    'Withdrawn': 0\n",
    "}\n",
    "\n",
    "data_dummies = pd.get_dummies(data, columns=['code_module', 'code_presentation', 'gender', 'region'])\n",
    "data_dummies['highest_education'] = data_dummies['highest_education'].map(highest_education)\n",
    "data_dummies['imd_band'] = data_dummies['imd_band'].map(imd_band)\n",
    "data_dummies['age_band'] = data_dummies['age_band'].map(age_band)\n",
    "data_dummies['disability'] = data_dummies['disability'].map(disability)\n",
    "data_dummies['studied_credits_binned'] = data_dummies['studied_credits_binned'].map(studied_credits_binned)\n",
    "data_dummies['final_result'] = data_dummies['final_result'].map(final_result)\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# data_ordinal = data.copy()\n",
    "# data_ordinal['highest_education'] = data_dummies['highest_education']\n",
    "# data_ordinal['imd_band'] = data_dummies['imd_band']\n",
    "# data_ordinal['age_band'] = data_dummies['age_band']\n",
    "# data_ordinal['disability'] = data_dummies['disability']\n",
    "# data_ordinal['studied_credits_binned'] = data_dummies['studied_credits_binned']\n",
    "# data_ordinal['final_result'] = data_dummies['final_result']\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# data_ordinal['code_module'] = le.fit_transform(data_ordinal['code_module'])\n",
    "# data_ordinal['code_presentation'] = le.fit_transform(data_ordinal['code_presentation'])\n",
    "# data_ordinal['gender'] = le.fit_transform(data_ordinal['gender'])\n",
    "# data_ordinal['region'] = le.fit_transform(data_ordinal['region'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.000000    55\n",
       "66.000000    52\n",
       "60.000000    52\n",
       "80.000000    50\n",
       "56.000000    49\n",
       "             ..\n",
       "63.890000     1\n",
       "55.262626     1\n",
       "52.204082     1\n",
       "54.185930     1\n",
       "54.989637     1\n",
       "Name: score, Length: 11668, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dummies['score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m y_dummy \u001b[38;5;241m=\u001b[39m data_dummies[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(sampling_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminority\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m X_dummy_smote, y_dummy_smote \u001b[38;5;241m=\u001b[39m \u001b[43msmote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_dummy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_dummy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m X_ord_smote, y_ord_smote \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(X_ord, y_ord)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/imblearn/base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/imblearn/base.py:104\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     \u001b[43mcheck_classification_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[1;32m    106\u001b[0m     X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:216\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    208\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    215\u001b[0m ]:\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Maybe you are trying to fit a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier, which expects discrete classes on a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression target with continuous values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE#pip install imbalanced-learn\n",
    "\n",
    "\n",
    "\n",
    "X_dummy = data_dummies.drop(['final_result', 'studied_credits', 'id_student', 'score', 'grade'], axis=1)\n",
    "y_dummy = data_dummies['score']\n",
    "\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "X_dummy_smote, y_dummy_smote = smote.fit_resample(X_dummy, y_dummy)\n",
    "X_ord_smote, y_ord_smote = smote.fit_resample(X_ord, y_ord)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_dummy_smote = pd.read_csv('data/X_dummy_smote.csv')\n",
    "y_dummy_smote = pd.read_csv('data/y_dummy_smote.csv')\n",
    "X_ord_smote = pd.read_csv('data/X_ord_smote.csv')\n",
    "y_ord_smote = pd.read_csv('data/y_ord_smote.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dummy, X_test_dummy, y_train_dummy, y_test_dummy = train_test_split(X_dummy_smote, y_dummy_smote, test_size=0.2, random_state=42)\n",
    "X_train_ord, X_test_ord, y_train_ord, y_test_ord = train_test_split(X_ord_smote, y_ord_smote, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(max_iter=10000)\n",
    "logreg.fit(X_train_dummy, y_train_dummy)\n",
    "\n",
    "y_pred = logreg.predict(X_test_dummy)\n",
    "acc = accuracy_score(y_test_dummy, y_pred)\n",
    "print('Accuracy Logistic Regression:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "from xgboost import XGBClassifier #pip install xgboost\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# Define the parameter space\n",
    "param_space = {\n",
    "    'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "    'min_child_weight': (1, 10),\n",
    "    'max_depth': (3, 50),\n",
    "    'max_delta_step': (1, 20),\n",
    "    'subsample': (0.01, 1.0, 'uniform'),\n",
    "    'colsample_bytree': (0.01, 1.0, 'uniform'),\n",
    "    'colsample_bylevel': (0.01, 1.0, 'uniform'),\n",
    "    'reg_lambda': (1e-9, 1000, 'log-uniform'),\n",
    "    'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n",
    "    'gamma': (1e-9, 0.5, 'log-uniform'),\n",
    "    'n_estimators': (50, 200),\n",
    "    'scale_pos_weight': (1e-6, 500, 'log-uniform')\n",
    "}\n",
    "\n",
    "# Create a BayesSearchCV object\n",
    "opt_xgb = BayesSearchCV(\n",
    "    estimator=XGBClassifier(n_jobs=-1),\n",
    "    search_spaces=param_space,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    n_iter=150,\n",
    "    verbose=1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# Run the optimization\n",
    "opt_xgb.fit(X_train_dummy, y_train_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train_dummy, y_train_dummy)\n",
    "\n",
    "y_pred = xgb.predict(X_test_dummy)\n",
    "acc = accuracy_score(y_test_dummy, y_pred)\n",
    "print(\"Accuracy XGBoost:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration 1\n",
      "Completed iteration 2\n",
      "Completed iteration 3\n",
      "Completed iteration 4\n",
      "Completed iteration 5\n",
      "Completed iteration 6\n",
      "Completed iteration 7\n",
      "Completed iteration 8\n",
      "Completed iteration 9\n",
      "Completed iteration 10\n",
      "Completed iteration 11\n",
      "Completed iteration 12\n",
      "Completed iteration 13\n",
      "Completed iteration 14\n",
      "Completed iteration 15\n",
      "Completed iteration 16\n",
      "Completed iteration 17\n",
      "Completed iteration 18\n",
      "Completed iteration 19\n",
      "Completed iteration 20\n",
      "Completed iteration 21\n",
      "Completed iteration 22\n",
      "Completed iteration 23\n",
      "Completed iteration 24\n",
      "Completed iteration 25\n",
      "Completed iteration 26\n",
      "Completed iteration 27\n",
      "Completed iteration 28\n",
      "Completed iteration 29\n",
      "Completed iteration 30\n",
      "Completed iteration 31\n",
      "Completed iteration 32\n",
      "Completed iteration 33\n",
      "Completed iteration 34\n",
      "Completed iteration 35\n",
      "Completed iteration 36\n",
      "Completed iteration 37\n",
      "Completed iteration 38\n",
      "Completed iteration 39\n",
      "Completed iteration 40\n",
      "Completed iteration 41\n",
      "Completed iteration 42\n",
      "Completed iteration 43\n",
      "Completed iteration 44\n",
      "Completed iteration 45\n",
      "Completed iteration 46\n",
      "Completed iteration 47\n",
      "Completed iteration 48\n",
      "Completed iteration 49\n",
      "Completed iteration 50\n",
      "Best parameters: OrderedDict([('bagging_temperature', 1.0), ('border_count', 148), ('depth', 13), ('iterations', 976), ('l2_leaf_reg', 30), ('learning_rate', 0.02389354323083735), ('random_strength', 1e-09), ('scale_pos_weight', 0.9510644977326121)])\n",
      "Best score: 0.7638817157088732\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "counter = [0]\n",
    "\n",
    "def on_step(optim_result):\n",
    "    # Increment the counter\n",
    "    counter[0] += 1\n",
    "    print(f\"Completed iteration {counter[0]}\")\n",
    "    \n",
    "cb = CatBoostClassifier(verbose=False)\n",
    "\n",
    "# Define search spaces\n",
    "param_space = {\n",
    "    'iterations': (50, 1000),\n",
    "    'depth': (1, 16),\n",
    "    'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "    'random_strength': (1e-9, 10, 'log-uniform'),\n",
    "    'bagging_temperature': (0.0, 1.0),\n",
    "    'border_count': (1, 255),\n",
    "    'l2_leaf_reg': (2, 30),\n",
    "    'scale_pos_weight':(0.01, 1.0, 'uniform')\n",
    "}\n",
    "\n",
    "# Initialize BayesSearchCV\n",
    "opt_cb = BayesSearchCV(\n",
    "    estimator=cb,\n",
    "    search_spaces=param_space,\n",
    "    scoring='accuracy',\n",
    "    n_iter=50,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=False,\n",
    "    refit=True,\n",
    "    optimizer_kwargs={'base_estimator': 'GP'},\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "opt_cb.fit(X_train_dummy, y_train_dummy, callback=on_step)\n",
    "\n",
    "# Print best parameters and score\n",
    "print(f'Best parameters: {opt_cb.best_params_}')\n",
    "print(f'Best score: {opt_cb.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "params = OrderedDict([('bagging_temperature', 1.0), ('border_count', 148), ('depth', 13), ('iterations', 976), ('l2_leaf_reg', 30), ('learning_rate', 0.02389354323083735), ('random_strength', 1e-09), ('scale_pos_weight', 0.9510644977326121)])\n",
    "params['iterations'] = 10000\n",
    "\n",
    "cb = CatBoostClassifier(**params)\n",
    "cb.fit(X_train_dummy, y_train_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy CatBoost: 0.7581110310021629\n"
     ]
    }
   ],
   "source": [
    "y_pred = cb.predict(X_test_dummy)\n",
    "acc = accuracy_score(y_test_dummy, y_pred)\n",
    "print('Accuracy CatBoost:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_dummy, y_train_dummy)\n",
    "\n",
    "y_pred = rf.predict(X_test_dummy)\n",
    "acc = accuracy_score(y_test_dummy, y_pred)\n",
    "print('Accuracy Random Forest:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "categories = ['code_module', 'code_presentation', 'gender', 'region']\n",
    "numerical_features = [col for col in X_train_ord.columns if col not in categories]\n",
    "\n",
    "X_train_dummy = scaler.fit_transform(X_train_dummy)\n",
    "X_test_dummy = scaler.fit_transform(X_test_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "input_layer = Input(shape=(X_train_dummy.shape[1],))\n",
    "\n",
    "dense = Dense(128, kernel_initializer=tf.keras.initializers.HeNormal())(input_layer)\n",
    "dense = BatchNormalization()(dense)\n",
    "dense = tf.keras.activations.relu(dense)\n",
    "dense = Dropout(0.2)(dense)\n",
    "\n",
    "dense = Dense(64, kernel_initializer=tf.keras.initializers.HeNormal())(dense)\n",
    "dense = BatchNormalization()(dense)\n",
    "dense = tf.keras.activations.relu(dense)\n",
    "dense = Dropout(0.2)(dense)\n",
    "\n",
    "dense = Dense(64, kernel_initializer=tf.keras.initializers.HeNormal())(dense)\n",
    "dense = BatchNormalization()(dense)\n",
    "dense = tf.keras.activations.relu(dense)\n",
    "dense = Dropout(0.2)(dense)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "model2 = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "hist2 = model2.fit(X_train_dummy, y_train_dummy, epochs=30, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(hist2.history['loss'])\n",
    "plt.plot(hist2.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN, TAKES FOREVER, OPTIMAL PARAMS PRINTED BELOW\n",
    "\n",
    "from skopt import BayesSearchCV #pip install scikit-optimize\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from joblib import parallel_backend\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "def create_model(neurons, dropout_rate, num_layers, learning_rate):\n",
    "    input_layer = Input(shape=(X_train_dummy.shape[1],))\n",
    "\n",
    "    dense = Dense(neurons, kernel_initializer=tf.keras.initializers.HeNormal())(input_layer)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = tf.keras.activations.relu(dense)\n",
    "    dense = Dropout(dropout_rate)(dense)\n",
    "    \n",
    "    for i in range(num_layers - 1):\n",
    "        dense = Dense(neurons, kernel_initializer=tf.keras.initializers.HeNormal())(dense)\n",
    "        dense = BatchNormalization()(dense)\n",
    "        dense = tf.keras.activations.relu(dense)\n",
    "        dense = Dropout(dropout_rate)(dense)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "wrapped_create_model = KerasClassifier(build_fn=create_model, verbose=1, epochs=25)\n",
    "\n",
    "param_space = {\n",
    "    'neurons': (8, 512),\n",
    "    'dropout_rate': (0.1, 0.5),\n",
    "    'num_layers': (1, 5),\n",
    "    'learning_rate': (0.0001, 0.1)\n",
    "}\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    wrapped_create_model,\n",
    "    param_space,\n",
    "    n_iter=50,\n",
    "    cv=3,\n",
    "    n_jobs=1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "opt.fit(X_train_dummy, y_train_dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = opt.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_parameters = opt.best_params_\n",
    "\n",
    "optimal_model = create_model(\n",
    "    neurons=optimal_parameters['neurons'],\n",
    "    dropout_rate=optimal_parameters['dropout_rate'],\n",
    "    num_layers=optimal_parameters['num_layers'],\n",
    "    learning_rate=optimal_parameters['learning_rate']\n",
    ")\n",
    "\n",
    "history = optimal_model.fit(X_train_dummy, y_train_dummy, epochs=30, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = optimal_model.evaluate(X_test_dummy, y_test_dummy)\n",
    "print(\"Accuracy (tuned model):\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
