{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/student_info_cleaned.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['final_result'].value_counts()\n",
    "#drop withdrawn students\n",
    "data = data[data['final_result'] != 'Withdrawn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#order highest_education, imd_band, age_band, disability, studied_credits_binned, final_result\n",
    "highest_education = {\n",
    "    'No Formal quals': 0,\n",
    "    'Lower Than A Level': 1,\n",
    "    'A Level or Equivalent': 2,\n",
    "    'HE Qualification': 3,\n",
    "    'Post Graduate Qualification': 4\n",
    "}\n",
    "\n",
    "imd_band = {\n",
    "    np.nan: -1,\n",
    "    '0-10%': 0,\n",
    "    '10-20': 1,\n",
    "    '20-30%': 2,\n",
    "    '30-40%': 3,\n",
    "    '40-50%': 4,\n",
    "    '50-60%': 5,\n",
    "    '60-70%': 6,\n",
    "    '70-80%': 7,\n",
    "    '80-90%': 8,\n",
    "    '90-100%': 9\n",
    "}\n",
    "\n",
    "age_band = {\n",
    "    '0-35': 0,\n",
    "    '35-55': 1,\n",
    "    '55<=': 2\n",
    "}\n",
    "\n",
    "disability = {\n",
    "    'N': 0,\n",
    "    'Y': 1\n",
    "}\n",
    "\n",
    "studied_credits_binned = {\n",
    "    '30-60': 0,\n",
    "    '61-100': 1,\n",
    "    '101-200': 2,\n",
    "    '201+': 3\n",
    "}\n",
    "\n",
    "final_result = {\n",
    "    'Fail': 0,\n",
    "    'Pass': 1,\n",
    "    'Distinction': 1,\n",
    "    'Withdrawn': 0\n",
    "}\n",
    "\n",
    "data_dummies = pd.get_dummies(data, columns=['code_module', 'code_presentation', 'gender', 'region'])\n",
    "data_dummies['highest_education'] = data_dummies['highest_education'].map(highest_education)\n",
    "data_dummies['imd_band'] = data_dummies['imd_band'].map(imd_band)\n",
    "data_dummies['age_band'] = data_dummies['age_band'].map(age_band)\n",
    "data_dummies['disability'] = data_dummies['disability'].map(disability)\n",
    "data_dummies['studied_credits_binned'] = data_dummies['studied_credits_binned'].map(studied_credits_binned)\n",
    "data_dummies['final_result'] = data_dummies['final_result'].map(final_result)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "data_ordinal = data.copy()\n",
    "data_ordinal['highest_education'] = data_dummies['highest_education']\n",
    "data_ordinal['imd_band'] = data_dummies['imd_band']\n",
    "data_ordinal['age_band'] = data_dummies['age_band']\n",
    "data_ordinal['disability'] = data_dummies['disability']\n",
    "data_ordinal['studied_credits_binned'] = data_dummies['studied_credits_binned']\n",
    "data_ordinal['final_result'] = data_dummies['final_result']\n",
    "\n",
    "le = LabelEncoder()\n",
    "data_ordinal['code_module'] = le.fit_transform(data_ordinal['code_module'])\n",
    "data_ordinal['code_presentation'] = le.fit_transform(data_ordinal['code_presentation'])\n",
    "data_ordinal['gender'] = le.fit_transform(data_ordinal['gender'])\n",
    "data_ordinal['region'] = le.fit_transform(data_ordinal['region'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummies['final_result'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE#pip install imbalanced-learn\n",
    "\n",
    "\n",
    "\n",
    "X_dummy = data_dummies.drop(['final_result', 'studied_credits', 'id_student', 'score', 'grade'], axis=1)\n",
    "y_dummy = data_dummies['final_result']\n",
    "\n",
    "X_ord = data_ordinal.drop(['final_result', 'studied_credits', 'id_student', 'score', 'grade'], axis=1)\n",
    "y_ord = data_ordinal['final_result']\n",
    "\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "X_dummy_smote, y_dummy_smote = smote.fit_resample(X_dummy, y_dummy)\n",
    "X_ord_smote, y_ord_smote = smote.fit_resample(X_ord, y_ord)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_dummy_smote = pd.read_csv('data/X_dummy_smote.csv')\n",
    "y_dummy_smote = pd.read_csv('data/y_dummy_smote.csv')\n",
    "X_ord_smote = pd.read_csv('data/X_ord_smote.csv')\n",
    "y_ord_smote = pd.read_csv('data/y_ord_smote.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dummy, X_test_dummy, y_train_dummy, y_test_dummy = train_test_split(X_dummy_smote, y_dummy_smote, test_size=0.2, random_state=42)\n",
    "X_train_ord, X_test_ord, y_train_ord, y_test_ord = train_test_split(X_ord_smote, y_ord_smote, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(max_iter=10000)\n",
    "logreg.fit(X_train_dummy, y_train_dummy)\n",
    "\n",
    "y_pred = logreg.predict(X_test_dummy)\n",
    "acc = accuracy_score(y_test_dummy, y_pred)\n",
    "print('Accuracy Logistic Regression:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "from xgboost import XGBClassifier #pip install xgboost\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# Define the parameter space\n",
    "param_space = {\n",
    "    'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "    'min_child_weight': (1, 10),\n",
    "    'max_depth': (3, 50),\n",
    "    'max_delta_step': (1, 20),\n",
    "    'subsample': (0.01, 1.0, 'uniform'),\n",
    "    'colsample_bytree': (0.01, 1.0, 'uniform'),\n",
    "    'colsample_bylevel': (0.01, 1.0, 'uniform'),\n",
    "    'reg_lambda': (1e-9, 1000, 'log-uniform'),\n",
    "    'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n",
    "    'gamma': (1e-9, 0.5, 'log-uniform'),\n",
    "    'n_estimators': (50, 200),\n",
    "    'scale_pos_weight': (1e-6, 500, 'log-uniform')\n",
    "}\n",
    "\n",
    "# Create a BayesSearchCV object\n",
    "opt_xgb = BayesSearchCV(\n",
    "    estimator=XGBClassifier(n_jobs=-1),\n",
    "    search_spaces=param_space,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    n_iter=150,\n",
    "    verbose=1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# Run the optimization\n",
    "opt_xgb.fit(X_train_dummy, y_train_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train_dummy, y_train_dummy)\n",
    "\n",
    "y_pred = xgb.predict(X_test_dummy)\n",
    "acc = accuracy_score(y_test_dummy, y_pred)\n",
    "print(\"Accuracy XGBoost:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317:\tlearn: 0.2622736\ttotal: 1m 22s\tremaining: 8.29s\n",
      "318:\tlearn: 0.2619867\ttotal: 1m 22s\tremaining: 8.03s\n",
      "319:\tlearn: 0.2617441\ttotal: 1m 22s\tremaining: 7.77s\n",
      "320:\tlearn: 0.2614217\ttotal: 1m 23s\tremaining: 7.51s\n",
      "321:\tlearn: 0.2609967\ttotal: 1m 23s\tremaining: 7.25s\n",
      "322:\tlearn: 0.2606959\ttotal: 1m 23s\tremaining: 6.99s\n",
      "323:\tlearn: 0.2604224\ttotal: 1m 23s\tremaining: 6.73s\n",
      "324:\tlearn: 0.2601207\ttotal: 1m 24s\tremaining: 6.47s\n",
      "325:\tlearn: 0.2597717\ttotal: 1m 24s\tremaining: 6.22s\n",
      "326:\tlearn: 0.2594467\ttotal: 1m 24s\tremaining: 5.96s\n",
      "327:\tlearn: 0.2592118\ttotal: 1m 24s\tremaining: 5.7s\n",
      "328:\tlearn: 0.2589252\ttotal: 1m 25s\tremaining: 5.44s\n",
      "329:\tlearn: 0.2585753\ttotal: 1m 25s\tremaining: 5.18s\n",
      "330:\tlearn: 0.2581954\ttotal: 1m 25s\tremaining: 4.92s\n",
      "331:\tlearn: 0.2578362\ttotal: 1m 25s\tremaining: 4.66s\n",
      "332:\tlearn: 0.2575411\ttotal: 1m 26s\tremaining: 4.4s\n",
      "333:\tlearn: 0.2571975\ttotal: 1m 26s\tremaining: 4.14s\n",
      "334:\tlearn: 0.2568485\ttotal: 1m 26s\tremaining: 3.88s\n",
      "335:\tlearn: 0.2564966\ttotal: 1m 26s\tremaining: 3.62s\n",
      "336:\tlearn: 0.2561532\ttotal: 1m 27s\tremaining: 3.37s\n",
      "337:\tlearn: 0.2558826\ttotal: 1m 27s\tremaining: 3.11s\n",
      "338:\tlearn: 0.2556144\ttotal: 1m 27s\tremaining: 2.85s\n",
      "339:\tlearn: 0.2553275\ttotal: 1m 28s\tremaining: 2.59s\n",
      "340:\tlearn: 0.2550306\ttotal: 1m 28s\tremaining: 2.33s\n",
      "341:\tlearn: 0.2547150\ttotal: 1m 28s\tremaining: 2.07s\n",
      "342:\tlearn: 0.2543855\ttotal: 1m 28s\tremaining: 1.81s\n",
      "343:\tlearn: 0.2540540\ttotal: 1m 29s\tremaining: 1.55s\n",
      "344:\tlearn: 0.2537404\ttotal: 1m 29s\tremaining: 1.29s\n",
      "345:\tlearn: 0.2535164\ttotal: 1m 29s\tremaining: 1.04s\n",
      "346:\tlearn: 0.2533156\ttotal: 1m 29s\tremaining: 777ms\n",
      "347:\tlearn: 0.2530483\ttotal: 1m 30s\tremaining: 518ms\n",
      "348:\tlearn: 0.2526773\ttotal: 1m 30s\tremaining: 259ms\n",
      "349:\tlearn: 0.2524929\ttotal: 1m 30s\tremaining: 0us\n",
      "Completed iteration 1\n",
      "317:\tlearn: 0.2650133\ttotal: 1m 26s\tremaining: 8.68s\n",
      "318:\tlearn: 0.2647186\ttotal: 1m 26s\tremaining: 8.4s\n",
      "319:\tlearn: 0.2643967\ttotal: 1m 26s\tremaining: 8.13s\n",
      "320:\tlearn: 0.2640160\ttotal: 1m 27s\tremaining: 7.86s\n",
      "321:\tlearn: 0.2636717\ttotal: 1m 27s\tremaining: 7.6s\n",
      "322:\tlearn: 0.2633685\ttotal: 1m 27s\tremaining: 7.33s\n",
      "323:\tlearn: 0.2630031\ttotal: 1m 27s\tremaining: 7.05s\n",
      "324:\tlearn: 0.2627305\ttotal: 1m 28s\tremaining: 6.78s\n",
      "325:\tlearn: 0.2624252\ttotal: 1m 28s\tremaining: 6.51s\n",
      "326:\tlearn: 0.2620858\ttotal: 1m 28s\tremaining: 6.24s\n",
      "327:\tlearn: 0.2617549\ttotal: 1m 29s\tremaining: 5.97s\n",
      "328:\tlearn: 0.2614989\ttotal: 1m 29s\tremaining: 5.71s\n",
      "329:\tlearn: 0.2611232\ttotal: 1m 29s\tremaining: 5.43s\n",
      "330:\tlearn: 0.2594025\ttotal: 1m 29s\tremaining: 5.16s\n",
      "331:\tlearn: 0.2591851\ttotal: 1m 30s\tremaining: 4.89s\n",
      "332:\tlearn: 0.2588937\ttotal: 1m 30s\tremaining: 4.62s\n",
      "333:\tlearn: 0.2585844\ttotal: 1m 30s\tremaining: 4.34s\n",
      "334:\tlearn: 0.2582193\ttotal: 1m 30s\tremaining: 4.07s\n",
      "335:\tlearn: 0.2579068\ttotal: 1m 31s\tremaining: 3.8s\n",
      "336:\tlearn: 0.2576014\ttotal: 1m 31s\tremaining: 3.53s\n",
      "337:\tlearn: 0.2573439\ttotal: 1m 31s\tremaining: 3.26s\n",
      "338:\tlearn: 0.2570504\ttotal: 1m 32s\tremaining: 2.98s\n",
      "339:\tlearn: 0.2568121\ttotal: 1m 32s\tremaining: 2.71s\n",
      "340:\tlearn: 0.2565338\ttotal: 1m 32s\tremaining: 2.44s\n",
      "341:\tlearn: 0.2561069\ttotal: 1m 32s\tremaining: 2.17s\n",
      "342:\tlearn: 0.2558033\ttotal: 1m 33s\tremaining: 1.9s\n",
      "343:\tlearn: 0.2554304\ttotal: 1m 33s\tremaining: 1.63s\n",
      "344:\tlearn: 0.2551984\ttotal: 1m 33s\tremaining: 1.35s\n",
      "345:\tlearn: 0.2549146\ttotal: 1m 33s\tremaining: 1.08s\n",
      "346:\tlearn: 0.2545246\ttotal: 1m 34s\tremaining: 813ms\n",
      "347:\tlearn: 0.2542762\ttotal: 1m 34s\tremaining: 542ms\n",
      "348:\tlearn: 0.2539778\ttotal: 1m 34s\tremaining: 271ms\n",
      "349:\tlearn: 0.2537785\ttotal: 1m 34s\tremaining: 0us\n",
      "Completed iteration 2\n",
      "Completed iteration 3\n",
      "Completed iteration 4\n",
      "Completed iteration 5\n",
      "Completed iteration 6\n",
      "Completed iteration 7\n",
      "Completed iteration 8\n",
      "Completed iteration 9\n",
      "Completed iteration 10\n",
      "Completed iteration 11\n",
      "317:\tlearn: 0.2646495\ttotal: 1m 23s\tremaining: 8.38s\n",
      "318:\tlearn: 0.2644031\ttotal: 1m 23s\tremaining: 8.12s\n",
      "319:\tlearn: 0.2640290\ttotal: 1m 23s\tremaining: 7.86s\n",
      "320:\tlearn: 0.2637507\ttotal: 1m 24s\tremaining: 7.6s\n",
      "321:\tlearn: 0.2634762\ttotal: 1m 24s\tremaining: 7.33s\n",
      "322:\tlearn: 0.2632357\ttotal: 1m 24s\tremaining: 7.07s\n",
      "323:\tlearn: 0.2629741\ttotal: 1m 24s\tremaining: 6.81s\n",
      "324:\tlearn: 0.2626466\ttotal: 1m 25s\tremaining: 6.55s\n",
      "325:\tlearn: 0.2623163\ttotal: 1m 25s\tremaining: 6.29s\n",
      "326:\tlearn: 0.2619677\ttotal: 1m 25s\tremaining: 6.03s\n",
      "327:\tlearn: 0.2616387\ttotal: 1m 25s\tremaining: 5.76s\n",
      "328:\tlearn: 0.2613021\ttotal: 1m 26s\tremaining: 5.5s\n",
      "329:\tlearn: 0.2609735\ttotal: 1m 26s\tremaining: 5.24s\n",
      "330:\tlearn: 0.2606413\ttotal: 1m 26s\tremaining: 4.98s\n",
      "331:\tlearn: 0.2602931\ttotal: 1m 27s\tremaining: 4.72s\n",
      "332:\tlearn: 0.2600439\ttotal: 1m 27s\tremaining: 4.45s\n",
      "333:\tlearn: 0.2596678\ttotal: 1m 27s\tremaining: 4.19s\n",
      "334:\tlearn: 0.2593951\ttotal: 1m 27s\tremaining: 3.93s\n",
      "335:\tlearn: 0.2591624\ttotal: 1m 28s\tremaining: 3.67s\n",
      "336:\tlearn: 0.2588853\ttotal: 1m 28s\tremaining: 3.41s\n",
      "337:\tlearn: 0.2585946\ttotal: 1m 28s\tremaining: 3.14s\n",
      "338:\tlearn: 0.2582533\ttotal: 1m 28s\tremaining: 2.88s\n",
      "339:\tlearn: 0.2579145\ttotal: 1m 29s\tremaining: 2.62s\n",
      "340:\tlearn: 0.2576220\ttotal: 1m 29s\tremaining: 2.36s\n",
      "341:\tlearn: 0.2574156\ttotal: 1m 29s\tremaining: 2.1s\n",
      "342:\tlearn: 0.2570424\ttotal: 1m 29s\tremaining: 1.83s\n",
      "343:\tlearn: 0.2568155\ttotal: 1m 30s\tremaining: 1.57s\n",
      "344:\tlearn: 0.2565838\ttotal: 1m 30s\tremaining: 1.31s\n",
      "345:\tlearn: 0.2563341\ttotal: 1m 30s\tremaining: 1.05s\n",
      "346:\tlearn: 0.2561025\ttotal: 1m 30s\tremaining: 787ms\n",
      "347:\tlearn: 0.2558318\ttotal: 1m 31s\tremaining: 524ms\n",
      "348:\tlearn: 0.2555639\ttotal: 1m 31s\tremaining: 262ms\n",
      "349:\tlearn: 0.2552524\ttotal: 1m 31s\tremaining: 0us\n",
      "317:\tlearn: 0.2630035\ttotal: 1m 27s\tremaining: 8.78s\n",
      "318:\tlearn: 0.2626066\ttotal: 1m 27s\tremaining: 8.5s\n",
      "319:\tlearn: 0.2622761\ttotal: 1m 27s\tremaining: 8.23s\n",
      "320:\tlearn: 0.2619607\ttotal: 1m 27s\tremaining: 7.95s\n",
      "321:\tlearn: 0.2616286\ttotal: 1m 28s\tremaining: 7.67s\n",
      "322:\tlearn: 0.2613063\ttotal: 1m 28s\tremaining: 7.4s\n",
      "323:\tlearn: 0.2610577\ttotal: 1m 28s\tremaining: 7.12s\n",
      "324:\tlearn: 0.2606706\ttotal: 1m 29s\tremaining: 6.85s\n",
      "325:\tlearn: 0.2603451\ttotal: 1m 29s\tremaining: 6.57s\n",
      "326:\tlearn: 0.2599486\ttotal: 1m 29s\tremaining: 6.3s\n",
      "327:\tlearn: 0.2597323\ttotal: 1m 29s\tremaining: 6.02s\n",
      "328:\tlearn: 0.2593707\ttotal: 1m 30s\tremaining: 5.75s\n",
      "329:\tlearn: 0.2590988\ttotal: 1m 30s\tremaining: 5.47s\n",
      "330:\tlearn: 0.2588001\ttotal: 1m 30s\tremaining: 5.2s\n",
      "331:\tlearn: 0.2584735\ttotal: 1m 30s\tremaining: 4.92s\n",
      "332:\tlearn: 0.2581430\ttotal: 1m 31s\tremaining: 4.65s\n",
      "333:\tlearn: 0.2578695\ttotal: 1m 31s\tremaining: 4.37s\n",
      "334:\tlearn: 0.2575906\ttotal: 1m 31s\tremaining: 4.1s\n",
      "335:\tlearn: 0.2572492\ttotal: 1m 31s\tremaining: 3.83s\n",
      "336:\tlearn: 0.2569486\ttotal: 1m 32s\tremaining: 3.55s\n",
      "337:\tlearn: 0.2566665\ttotal: 1m 32s\tremaining: 3.28s\n",
      "338:\tlearn: 0.2564030\ttotal: 1m 32s\tremaining: 3s\n",
      "339:\tlearn: 0.2560706\ttotal: 1m 32s\tremaining: 2.73s\n",
      "340:\tlearn: 0.2557573\ttotal: 1m 33s\tremaining: 2.46s\n",
      "341:\tlearn: 0.2554695\ttotal: 1m 33s\tremaining: 2.18s\n",
      "342:\tlearn: 0.2551460\ttotal: 1m 33s\tremaining: 1.91s\n",
      "343:\tlearn: 0.2549123\ttotal: 1m 33s\tremaining: 1.64s\n",
      "344:\tlearn: 0.2545784\ttotal: 1m 34s\tremaining: 1.36s\n",
      "345:\tlearn: 0.2543113\ttotal: 1m 34s\tremaining: 1.09s\n",
      "346:\tlearn: 0.2540361\ttotal: 1m 34s\tremaining: 818ms\n",
      "347:\tlearn: 0.2537568\ttotal: 1m 34s\tremaining: 545ms\n",
      "348:\tlearn: 0.2535231\ttotal: 1m 35s\tremaining: 272ms\n",
      "349:\tlearn: 0.2532908\ttotal: 1m 35s\tremaining: 0us\n",
      "Completed iteration 12\n",
      "Completed iteration 13\n",
      "Completed iteration 14\n",
      "Completed iteration 15\n",
      "Completed iteration 16\n",
      "Completed iteration 17\n",
      "Completed iteration 18\n",
      "Completed iteration 19\n",
      "Completed iteration 20\n",
      "Completed iteration 21\n",
      "Completed iteration 22\n",
      "Completed iteration 23\n",
      "Completed iteration 24\n",
      "Completed iteration 25\n",
      "Completed iteration 26\n",
      "Completed iteration 27\n",
      "Completed iteration 28\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m opt_cb \u001b[38;5;241m=\u001b[39m BayesSearchCV(\n\u001b[1;32m     27\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mcb,\n\u001b[1;32m     28\u001b[0m     search_spaces\u001b[38;5;241m=\u001b[39mparam_space,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mopt_cb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_dummy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_dummy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Print best parameters and score\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopt_cb\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/skopt/searchcv.py:538\u001b[0m, in \u001b[0;36mBayesSearchCV.fit\u001b[0;34m(self, X, y, groups, callback, **fit_params)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefit):\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBayesSearchCV doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support a callable refit, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt define an implicit score to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    535\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    536\u001b[0m     )\n\u001b[0;32m--> 538\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;66;03m# BaseSearchCV never ranked train scores,\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# but apparently we used to ship this (back-compat)\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_train_score:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/skopt/searchcv.py:595\u001b[0m, in \u001b[0;36mBayesSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n_iter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;66;03m# when n_iter < n_points points left for evaluation\u001b[39;00m\n\u001b[1;32m    593\u001b[0m     n_points_adjusted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_iter, n_points)\n\u001b[0;32m--> 595\u001b[0m     optim_result, score_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_points_adjusted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m     n_iter \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m n_points\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_callbacks(callbacks, optim_result):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/skopt/searchcv.py:449\u001b[0m, in \u001b[0;36mBayesSearchCV._step\u001b[0;34m(self, search_space, optimizer, score_name, evaluate_candidates, n_points)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# make lists into dictionaries\u001b[39;00m\n\u001b[1;32m    447\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m [point_asdict(search_space, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params]\n\u001b[0;32m--> 449\u001b[0m all_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# if self.scoring is a callable, we have to wait until here\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# to get the score name\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "counter = [0]\n",
    "\n",
    "def on_step(optim_result):\n",
    "    # Increment the counter\n",
    "    counter[0] += 1\n",
    "    print(f\"Completed iteration {counter[0]}\")\n",
    "    \n",
    "cb = CatBoostClassifier(verbose=False)\n",
    "\n",
    "# Define search spaces\n",
    "param_space = {\n",
    "    'iterations': (50, 1000),\n",
    "    'depth': (1, 16),\n",
    "    'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "    'random_strength': (1e-9, 10, 'log-uniform'),\n",
    "    'bagging_temperature': (0.0, 1.0),\n",
    "    'border_count': (1, 255),\n",
    "    'l2_leaf_reg': (2, 30),\n",
    "    'scale_pos_weight':(0.01, 1.0, 'uniform')\n",
    "}\n",
    "\n",
    "# Initialize BayesSearchCV\n",
    "opt_cb = BayesSearchCV(\n",
    "    estimator=cb,\n",
    "    search_spaces=param_space,\n",
    "    scoring='accuracy',\n",
    "    n_iter=50,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=False,\n",
    "    refit=True,\n",
    "    optimizer_kwargs={'base_estimator': 'GP'},\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "opt_cb.fit(X_train_dummy, y_train_dummy, callback=on_step)\n",
    "\n",
    "# Print best parameters and score\n",
    "print(f'Best parameters: {opt_cb.best_params_}')\n",
    "print(f'Best score: {opt_cb.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_dummy, y_train_dummy)\n",
    "\n",
    "y_pred = rf.predict(X_test_dummy)\n",
    "acc = accuracy_score(y_test_dummy, y_pred)\n",
    "print('Accuracy Random Forest:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "categories = ['code_module', 'code_presentation', 'gender', 'region']\n",
    "numerical_features = [col for col in X_train_ord.columns if col not in categories]\n",
    "\n",
    "X_train_dummy = scaler.fit_transform(X_train_dummy)\n",
    "X_test_dummy = scaler.fit_transform(X_test_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "input_layer = Input(shape=(X_train_dummy.shape[1],))\n",
    "\n",
    "dense = Dense(128, kernel_initializer=tf.keras.initializers.HeNormal())(input_layer)\n",
    "dense = BatchNormalization()(dense)\n",
    "dense = tf.keras.activations.relu(dense)\n",
    "dense = Dropout(0.2)(dense)\n",
    "\n",
    "dense = Dense(64, kernel_initializer=tf.keras.initializers.HeNormal())(dense)\n",
    "dense = BatchNormalization()(dense)\n",
    "dense = tf.keras.activations.relu(dense)\n",
    "dense = Dropout(0.2)(dense)\n",
    "\n",
    "dense = Dense(64, kernel_initializer=tf.keras.initializers.HeNormal())(dense)\n",
    "dense = BatchNormalization()(dense)\n",
    "dense = tf.keras.activations.relu(dense)\n",
    "dense = Dropout(0.2)(dense)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "model2 = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "hist2 = model2.fit(X_train_dummy, y_train_dummy, epochs=30, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(hist2.history['loss'])\n",
    "plt.plot(hist2.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN, TAKES FOREVER, OPTIMAL PARAMS PRINTED BELOW\n",
    "\n",
    "from skopt import BayesSearchCV #pip install scikit-optimize\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from joblib import parallel_backend\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "def create_model(neurons, dropout_rate, num_layers, learning_rate):\n",
    "    input_layer = Input(shape=(X_train_dummy.shape[1],))\n",
    "\n",
    "    dense = Dense(neurons, kernel_initializer=tf.keras.initializers.HeNormal())(input_layer)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = tf.keras.activations.relu(dense)\n",
    "    dense = Dropout(dropout_rate)(dense)\n",
    "    \n",
    "    for i in range(num_layers - 1):\n",
    "        dense = Dense(neurons, kernel_initializer=tf.keras.initializers.HeNormal())(dense)\n",
    "        dense = BatchNormalization()(dense)\n",
    "        dense = tf.keras.activations.relu(dense)\n",
    "        dense = Dropout(dropout_rate)(dense)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "wrapped_create_model = KerasClassifier(build_fn=create_model, verbose=1, epochs=25)\n",
    "\n",
    "param_space = {\n",
    "    'neurons': (8, 512),\n",
    "    'dropout_rate': (0.1, 0.5),\n",
    "    'num_layers': (1, 5),\n",
    "    'learning_rate': (0.0001, 0.1)\n",
    "}\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    wrapped_create_model,\n",
    "    param_space,\n",
    "    n_iter=50,\n",
    "    cv=3,\n",
    "    n_jobs=1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "opt.fit(X_train_dummy, y_train_dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = opt.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_parameters = opt.best_params_\n",
    "\n",
    "optimal_model = create_model(\n",
    "    neurons=optimal_parameters['neurons'],\n",
    "    dropout_rate=optimal_parameters['dropout_rate'],\n",
    "    num_layers=optimal_parameters['num_layers'],\n",
    "    learning_rate=optimal_parameters['learning_rate']\n",
    ")\n",
    "\n",
    "history = optimal_model.fit(X_train_dummy, y_train_dummy, epochs=30, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = optimal_model.evaluate(X_test_dummy, y_test_dummy)\n",
    "print(\"Accuracy (tuned model):\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
